#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ ETL Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ)

ğŸ”§ ØªØºÛŒÛŒØ±Ø§Øª Ù…Ù‡Ù… Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡:
- âœ… Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ fetch_01_fixed_clean.py
- âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø¬Ø¯ÛŒØ¯ (Reddit, NewsAPI, RSS, CoinGecko)
- âœ… ØªØ´Ø®ÛŒØµ Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¬Ø¯ÛŒØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
- âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ API sources Ù…Ø®ØªÙ„Ù
- âœ… Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¹Ø¯Ù… ØªØ·Ø¨ÛŒÙ‚ Ø²Ù…Ø§Ù†ÛŒ Ø¨ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ùˆ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§
- âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Broadcasting Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¯ÙˆØ±Ù‡
- âœ… Ø§ØµÙ„Ø§Ø­ Ù…Ù†Ø·Ù‚ Ø§Ø¯ØºØ§Ù… Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø§Ø­Ø³Ø§Ø³Ø§Øª
- âœ… Ø§ÙØ²ÙˆØ¯Ù† fallback Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§Øª

ØªØºÛŒÛŒØ±Ø§Øª Ø§ØµÙ„ÛŒ:
- Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¹Ø¯Ù… ØªØ·Ø¨ÛŒÙ‚ Ø²Ù…Ø§Ù†ÛŒ Ø¨ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ùˆ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Broadcasting Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¯ÙˆØ±Ù‡
- Ø§ØµÙ„Ø§Ø­ Ù…Ù†Ø·Ù‚ Ø§Ø¯ØºØ§Ù… Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø§Ø­Ø³Ø§Ø³Ø§Øª
- Ø§ÙØ²ÙˆØ¯Ù† fallback Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§Øª
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø¬Ø¯ÛŒØ¯
"""

import os
import re
import glob
import pandas as pd
import logging
import configparser
from typing import List, Dict, Optional, Tuple, Any
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from datetime import datetime
import numpy as np

# Ø¨Ø®Ø´ Ø®ÙˆØ§Ù†Ø¯Ù† Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
config = configparser.ConfigParser()
CONFIG_FILE_PATH = 'config.ini'
try:
    config.read(CONFIG_FILE_PATH, encoding='utf-8')
    RAW_DATA_PATH = config.get('Paths', 'raw')
    PROCESSED_DATA_PATH = config.get('Paths', 'processed')
    LOG_PATH = config.get('Paths', 'logs')
except Exception as e:
    print(f"CRITICAL ERROR: Could not read 'config.ini'. Error: {e}")
    exit()

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ ---
script_name = os.path.splitext(os.path.basename(__file__))[0]
log_subfolder_path = os.path.join(LOG_PATH, script_name)
os.makedirs(log_subfolder_path, exist_ok=True)

log_filename = os.path.join(log_subfolder_path, f"log_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])

# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
os.makedirs(RAW_DATA_PATH, exist_ok=True)
os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)
os.makedirs(LOG_PATH, exist_ok=True)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø¯ÙˆÙ† Ø²ÛŒØ±Ù¾ÙˆØ´Ù‡
price_raw_path = RAW_DATA_PATH
news_raw_path = RAW_DATA_PATH
processed_price_path = PROCESSED_DATA_PATH
processed_sentiment_path = PROCESSED_DATA_PATH

# --- Ú©Ù„Ø§Ø³ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¯Ø§Ø¯Ù‡ (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡) ---
class UnifiedDataProcessor:
    """Ú©Ù„Ø§Ø³ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
    
    def __init__(self):
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        self.price_data = None
        self.sentiment_data = None
        
        # === Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ ===
        self.known_news_sources = {
            'GNews': 'gnews',
            'NewsAPI': 'newsapi', 
            'CoinGecko': 'coingecko',
            'RSS': 'rss',
            'Reddit': 'reddit'
        }
        
        logging.info("ğŸš€ Enhanced Unified Data Processor Ø§ÙˆÙ„ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯")
        logging.info(f"ğŸ”— Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ: {list(self.known_news_sources.keys())}")
    
    def debug_timestamp_column(self, df: pd.DataFrame, context: str = ""):
        """ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ debug Ú©Ø±Ø¯Ù† Ù…Ø´Ú©Ù„Ø§Øª timestamp"""
        logging.info(f"ğŸ” Debug timestamp {context}:")
        logging.info(f"   Ù†ÙˆØ¹: {df['timestamp'].dtype}")
        logging.info(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„: {len(df)}")
        logging.info(f"   ØªØ¹Ø¯Ø§Ø¯ null: {df['timestamp'].isnull().sum()}")
        
        if len(df) > 0:
            logging.info(f"   Ù†Ù…ÙˆÙ†Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ±: {df['timestamp'].head(3).tolist()}")
            logging.info(f"   Ù…Ø­Ø¯ÙˆØ¯Ù‡: {df['timestamp'].min()} ØªØ§ {df['timestamp'].max()}")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ø³ØªÙˆÙ†
        unique_types = df['timestamp'].apply(type).value_counts()
        logging.info(f"   Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡: {unique_types.to_dict()}")
    
    def extract_metadata_from_filename(self, filename: str) -> Tuple[str, str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§Ø¯ Ùˆ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        basename = os.path.basename(filename).upper()
        symbol, timeframe = "UNKNOWN", "UNKNOWN"
        
        # Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§Ø¯
        symbol_match = re.search(r'([A-Z0-9]{2,}[-_]?(USDT|USD|BUSD|BTC|ETH|BNB|USDC|DAI))', basename)
        if symbol_match:
            # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ø³Ù„Ø´
            symbol = symbol_match.group(1).replace('-', '/').replace('_', '/')
            if '/' not in symbol: # Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ BTCUSDT
                quote = symbol_match.group(2)
                base = symbol.replace(quote, '')
                symbol = f"{base}/{quote}"
        
        # Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…
        tf_match = re.search(r'(\d+[MHWD])|HISTO(?:MINUTE|HOUR|DAY)', basename)
        if tf_match:
            timeframe = tf_match.group(0).replace("HISTOMINUTE", "1m").replace("HISTOHOUR", "1h").replace("HISTODAY", "1d")
        
        return symbol, timeframe
    
    def is_price_file(self, filename: str) -> bool:
        """ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        basename = os.path.basename(filename).lower()
        
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‚Ø·Ø¹Ø§Ù‹ Ù‚ÛŒÙ…Øª Ù†ÛŒØ³ØªÙ†Ø¯
        if basename.startswith('news_') or basename.startswith('sentiment_') or \
           basename.startswith('unified_extraction_state') or \
           'sentiment' in basename or 'news' in basename:
            return False
        
        # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù‚ÛŒÙ…Øª Ù‡Ø³ØªÙ†Ø¯
        price_indicators = [
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØµØ±Ø§ÙÛŒ
            'binance_', 'cryptocompare_', 'kraken_',
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…
            '_1m_', '_5m_', '_15m_', '_1h_', '_4h_', '_1d_',
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù„ÛŒ
            'ohlc', 'candle', 'kline', 'price'
        ]
        
        return any(indicator in basename for indicator in price_indicators)
    
    def is_news_file(self, filename: str) -> bool:
        """ØªØ´Ø®ÛŒØµ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        basename = os.path.basename(filename).lower()
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ
        news_patterns = [
            'news_',           # ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯: news_BTC-USDT_en_20241127_143022.csv
            'raw_news_',       # ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ…ÛŒ
            'sentiment_',      # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        ]
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        if any(basename.startswith(pattern) for pattern in news_patterns):
            return True
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø¯Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„
        news_source_indicators = [source.lower() for source in self.known_news_sources.values()]
        if any(source in basename for source in news_source_indicators):
            return True
        
        return False
    
    def standardize_price_data(self, df: pd.DataFrame, filename: str) -> Optional[pd.DataFrame]:
        """Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª"""
        df_copy = df.copy()
        df_copy.columns = df_copy.columns.str.lower().str.strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§
        symbol_from_col = df_copy['symbol'].iloc[0] if 'symbol' in df_copy.columns else None
        timeframe_from_col = df_copy['timeframe'].iloc[0] if 'timeframe' in df_copy.columns else None
        
        symbol_from_fname, timeframe_from_fname = self.extract_metadata_from_filename(filename)
        
        # Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ ÙØ§ÛŒÙ„
        final_symbol = symbol_from_col or symbol_from_fname
        final_timeframe = timeframe_from_col or timeframe_from_fname
        
        # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ OHLCV
        column_map: Dict[str, str] = {
            'time': 'timestamp', 'date': 'timestamp', 'datetime': 'timestamp', 'unnamed: 0': 'timestamp',
            'open': 'open', 'high': 'high', 'low': 'low', 'close': 'close', 'price': 'close',
            'volume': 'volume', 'volumefrom': 'volume'
        }
        df_copy.rename(columns=column_map, inplace=True)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ timestamp
        if 'timestamp' in df_copy.columns:
            df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'], errors='coerce', unit='s').fillna(
                pd.to_datetime(df_copy['timestamp'], errors='coerce')
            )
            df_copy.dropna(subset=['timestamp'], inplace=True)
            df_copy.set_index('timestamp', inplace=True)
        elif isinstance(df_copy.index, pd.DatetimeIndex):
            pass
        else:
            return None
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
        if 'close' not in df_copy.columns: 
            return None
        
        # ØªÚ©Ù…ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ OHLC
        for col in ['open', 'high', 'low']:
            if col not in df_copy.columns: 
                df_copy[col] = df_copy['close']
        
        if 'volume' not in df_copy.columns: 
            df_copy['volume'] = 0
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numeric
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')
        
        df_copy.dropna(subset=['open', 'high', 'low', 'close'], inplace=True)
        
        # Ø§ÙØ²ÙˆØ¯Ù† Ù…ØªØ§Ø¯ÛŒØªØ§
        df_copy['symbol'] = final_symbol
        df_copy['timeframe'] = final_timeframe
        
        return df_copy[['symbol', 'timeframe', 'open', 'high', 'low', 'close', 'volume']]
    
    def process_price_data(self) -> pd.DataFrame:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        logging.info("Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª...")
        
        # ÛŒØ§ÙØªÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ø§ ÙÛŒÙ„ØªØ± Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        all_files = glob.glob(os.path.join(RAW_DATA_PATH, '*.*'))        
        price_files = []
        
        for f_path in all_files:
            if f_path.endswith(('.csv', '.json', '.parquet')) and self.is_price_file(f_path):
                price_files.append(f_path)
        
        logging.info(f"ØªØ¹Ø¯Ø§Ø¯ {len(price_files)} ÙØ§ÛŒÙ„ Ù‚ÛŒÙ…Øª ÛŒØ§ÙØª Ø´Ø¯")
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        if price_files:
            logging.info("Ù†Ù…ÙˆÙ†Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª ÛŒØ§ÙØª Ø´Ø¯Ù‡:")
            for f_path in price_files[:5]:  # Ù†Ù…Ø§ÛŒØ´ 5 ÙØ§ÛŒÙ„ Ø§ÙˆÙ„
                logging.info(f"   - {os.path.basename(f_path)}")
            if len(price_files) > 5:
                logging.info(f"   ... Ùˆ {len(price_files) - 5} ÙØ§ÛŒÙ„ Ø¯ÛŒÚ¯Ø±")
        
        all_dataframes = []
        
        for f_path in price_files:
            try:
                # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„
                if f_path.endswith('.csv'):
                    df = pd.read_csv(f_path, low_memory=False)
                elif f_path.endswith('.json'):
                    df = pd.read_json(f_path, lines=True)
                elif f_path.endswith('.parquet'):
                    df = pd.read_parquet(f_path)
                else:
                    continue
                
                # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ
                validated_df = self.standardize_price_data(df, f_path)
                
                if validated_df is not None and not validated_df.empty:
                    all_dataframes.append(validated_df)
                    logging.info(f"âœ… ÙØ§ÛŒÙ„ '{os.path.basename(f_path)}' Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯ "
                               f"(Ù†Ù…Ø§Ø¯: {validated_df['symbol'].iloc[0]}, "
                               f"ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…: {validated_df['timeframe'].iloc[0]})")
            
            except Exception as e:
                logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ '{os.path.basename(f_path)}': {e}")
        
        if not all_dataframes:
            logging.error("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¯ØºØ§Ù… ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return pd.DataFrame()
        
        # Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        logging.info("Ø´Ø±ÙˆØ¹ Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª...")
        master_df = pd.concat(all_dataframes)
        
        # Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ UNKNOWN
        master_df = master_df[(master_df['symbol'] != 'UNKNOWN') & (master_df['timeframe'] != 'UNKNOWN')]
        
        # ØªÙ†Ø¸ÛŒÙ… index Ú†Ù†Ø¯ Ø³Ø·Ø­ÛŒ
        master_df.set_index(['symbol', 'timeframe'], append=True, inplace=True)
        master_df = master_df.reorder_levels(['symbol', 'timeframe', 'timestamp'])
        master_df.sort_index(inplace=True)
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        master_df = master_df[~master_df.index.duplicated(keep='last')]
        
        logging.info(f"âœ… Ø§Ø¯ØºØ§Ù… Ù‚ÛŒÙ…Øª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {len(master_df)}")
        
        self.price_data = master_df
        return master_df
    
    def analyze_sentiment_with_vader(self, text: str) -> Dict[str, float]:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒÚ© Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² VADER"""
        try:
            if not text or not isinstance(text, str):
                return {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}
            
            scores = self.sentiment_analyzer.polarity_scores(text)
            return scores
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª: {e}")
            return {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}
    
    def detect_news_source(self, file_path: str, df: pd.DataFrame) -> str:
        """ØªØ´Ø®ÛŒØµ Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„ ÛŒØ§ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„"""
        basename = os.path.basename(file_path).lower()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ù… ÙØ§ÛŒÙ„
        for source_name, source_key in self.known_news_sources.items():
            if source_key in basename:
                return source_name
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ† api_source Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        if 'api_source' in df.columns:
            sources = df['api_source'].value_counts()
            if not sources.empty:
                return sources.index[0]  # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù¾Ø±ØªÚ©Ø±Ø§Ø±ØªØ±ÛŒÙ† Ù…Ù†Ø¨Ø¹
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ† source
        if 'source' in df.columns:
            sources = df['source'].value_counts()
            if not sources.empty:
                # ØªØ´Ø®ÛŒØµ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡
                top_source = sources.index[0].lower()
                if 'reddit' in top_source or 'r/' in top_source:
                    return 'Reddit'
                elif 'newsapi' in top_source:
                    return 'NewsAPI'
                elif 'coingecko' in top_source:
                    return 'CoinGecko'
                elif any(rss_name in top_source for rss_name in ['coindesk', 'cointelegraph', 'decrypt', 'cryptonews']):
                    return 'RSS'
        
        return 'Unknown'
    
    def process_news_file(self, file_path: str) -> Optional[pd.DataFrame]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© ÙØ§ÛŒÙ„ Ø®Ø¨Ø±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¢Ù† - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        try:
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path, encoding='utf-8-sig')
            elif file_path.endswith('.parquet'):
                df = pd.read_parquet(file_path)
            else:
                return None
            
            # ØªØ´Ø®ÛŒØµ Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ
            news_source = self.detect_news_source(file_path, df)
            
            logging.info(f"Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø®Ø¨Ø±ÛŒ: {os.path.basename(file_path)} Ø¨Ø§ {len(df)} Ø®Ø¨Ø± (Ù…Ù†Ø¨Ø¹: {news_source})")
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
            required_cols = ['timestamp', 'symbol', 'title']
            if not all(col in df.columns for col in required_cols):
                logging.error(f"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¯Ø± ÙØ§ÛŒÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯: {required_cols}")
                logging.error(f"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {list(df.columns)}")
                return None
            
            # ØªØ±Ú©ÛŒØ¨ Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹â€ŒØªØ±
            df['full_text'] = (
                df['title'].fillna('') + ". " + 
                df.get('content', '').fillna('') + ". " + 
                df.get('description', '').fillna('')
            )
            
            # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…ØªÙ† Ù…Ø¹ØªØ¨Ø±
            df = df[df['full_text'].str.strip().str.len() > 10]
            
            if df.empty:
                return None
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù†Ø¨Ø¹
            df['detected_source'] = news_source
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª
            sentiment_scores = df['full_text'].apply(lambda x: self.analyze_sentiment_with_vader(x))
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
            df['sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])
            df['sentiment_positive'] = sentiment_scores.apply(lambda x: x['pos'])
            df['sentiment_negative'] = sentiment_scores.apply(lambda x: x['neg'])
            df['sentiment_neutral'] = sentiment_scores.apply(lambda x: x['neu'])
            
            # ØªØ¹ÛŒÛŒÙ† Ø¨Ø±Ú†Ø³Ø¨ Ø§Ø­Ø³Ø§Ø³Ø§Øª
            df['sentiment_label'] = df['sentiment_compound'].apply(
                lambda x: 'positive' if x >= 0.05 else ('negative' if x <= -0.05 else 'neutral')
            )
            
            # ØªØ¨Ø¯ÛŒÙ„ timestamp Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ±
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', utc=True)
            df = df.dropna(subset=['timestamp'])

            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†ÙˆØ¹ datetime
            if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                logging.error(f"Ø®Ø·Ø§: Ø³ØªÙˆÙ† timestamp Ù†ÙˆØ¹ datetime Ù†Ø¯Ø§Ø±Ø¯ Ø¯Ø± ÙØ§ÛŒÙ„ {file_path}")
                return None

            # ØªØ¨Ø¯ÛŒÙ„ timezone-aware Ø¨Ù‡ naive Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯
            if df['timestamp'].dt.tz is not None:
                df['timestamp'] = df['timestamp'].dt.tz_convert('UTC').dt.tz_localize(None)

            logging.info(f"âœ… timestamp ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯: {df['timestamp'].dtype}, Ù…Ø­Ø¯ÙˆØ¯Ù‡: {df['timestamp'].min()} ØªØ§ {df['timestamp'].max()}")
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
            df['text_length'] = df['full_text'].str.len()
            df['title_length'] = df['title'].str.len()
            
            # Ø§Ú¯Ø± sentiment_score Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª (Ø§Ø² Ø§Ø³Ú©Ø±ÛŒÙ¾Øª 01 ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡)
            if 'sentiment_score' in df.columns:
                # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¯Ùˆ Ø±ÙˆØ´
                df['sentiment_compound'] = (df['sentiment_compound'] + df['sentiment_score']) / 2
            
            # === ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ Reddit ===
            if news_source == 'Reddit':
                # Reddit Ø¯Ø§Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø§Ø³Øª
                if 'score' in df.columns:
                    df['reddit_score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0)
                if 'comments' in df.columns:
                    df['reddit_comments'] = pd.to_numeric(df['comments'], errors='coerce').fillna(0)
                
                logging.info(f"âœ… Reddit features Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
            
            return df
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ {file_path}: {e}")
            return None
    
    def process_sentiment_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ø§Ø­Ø³Ø§Ø³Ø§Øª - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        logging.info("Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        
        # ÛŒØ§ÙØªÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø¨Ø§ ÙÛŒÙ„ØªØ± Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        all_files = glob.glob(os.path.join(RAW_DATA_PATH, '*.*'))
        news_files = []
        
        for f_path in all_files:
            if f_path.endswith(('.csv', '.parquet')) and self.is_news_file(f_path):
                news_files.append(f_path)
        
        if not news_files:
            logging.warning("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø®Ø¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
        
        logging.info(f"ØªØ¹Ø¯Ø§Ø¯ {len(news_files)} ÙØ§ÛŒÙ„ Ø®Ø¨Ø±ÛŒ ÛŒØ§ÙØª Ø´Ø¯")
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        if news_files:
            logging.info("Ù†Ù…ÙˆÙ†Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡:")
            for f_path in news_files[:5]:  # Ù†Ù…Ø§ÛŒØ´ 5 ÙØ§ÛŒÙ„ Ø§ÙˆÙ„
                logging.info(f"   - {os.path.basename(f_path)}")
            if len(news_files) > 5:
                logging.info(f"   ... Ùˆ {len(news_files) - 5} ÙØ§ÛŒÙ„ Ø¯ÛŒÚ¯Ø±")
        
        all_processed_dfs = []
        source_stats = {}
        
        for file_path in news_files:
            processed_df = self.process_news_file(file_path)
            if processed_df is not None:
                all_processed_dfs.append(processed_df)
                
                # Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹
                if 'detected_source' in processed_df.columns:
                    source = processed_df['detected_source'].iloc[0]
                    source_stats[source] = source_stats.get(source, 0) + len(processed_df)
        
        if not all_processed_dfs:
            logging.error("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹
        logging.info("\nğŸ“Š Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ:")
        for source, count in source_stats.items():
            logging.info(f"   ğŸ“¡ {source}: {count:,} Ø®Ø¨Ø±")
        
        # Ø§Ø¯ØºØ§Ù… ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
        logging.info("Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        combined_df = pd.concat(all_processed_dfs, ignore_index=True)

        # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø¹Ø¯ Ø§Ø² concat
        logging.info(f"Ù†ÙˆØ¹ Ø³ØªÙˆÙ† timestamp Ø¨Ø¹Ø¯ Ø§Ø² concat: {combined_df['timestamp'].dtype}")

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÛŒÚ©Ø³Ø§Ù† Ø¨ÙˆØ¯Ù† Ù†ÙˆØ¹ timestamp Ø¯Ø± ØªÙ…Ø§Ù… Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§
        if combined_df['timestamp'].dtype == 'object':
            logging.warning("Ù†ÙˆØ¹ timestamp object Ø§Ø³Øª - ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ datetime...")
            combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], errors='coerce')
            
            # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ timestamp Ø¢Ù†â€ŒÙ‡Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù†Ø´Ø¯Ù‡
            before_drop = len(combined_df)
            combined_df = combined_df.dropna(subset=['timestamp'])
            after_drop = len(combined_df)
            
            if before_drop != after_drop:
                logging.warning(f"Ø­Ø°Ù {before_drop - after_drop} Ø±Ø¯ÛŒÙ Ø¨Ø§ timestamp Ù†Ø§Ù…Ø¹ØªØ¨Ø±")
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        before_dedup = len(combined_df)
        dedup_cols = ['timestamp', 'symbol', 'title']
        
        # Ø§Ú¯Ø± Ø³ØªÙˆÙ† url Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ù‡Ù… Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
        if 'url' in combined_df.columns:
            dedup_cols.append('url')
        
        combined_df = combined_df.drop_duplicates(subset=dedup_cols)
        after_dedup = len(combined_df)
        
        if before_dedup != after_dedup:
            logging.info(f"Ø­Ø°Ù {before_dedup - after_dedup} Ø±Ø¯ÛŒÙ ØªÚ©Ø±Ø§Ø±ÛŒ")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ timestamp Ù‚Ø¨Ù„ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
        logging.info(f"Ù†ÙˆØ¹ Ø³ØªÙˆÙ† timestamp Ù‚Ø¨Ù„ Ø§Ø² ØªØ¬Ù…ÛŒØ¹: {combined_df['timestamp'].dtype}")
        logging.info(f"Ù†Ù…ÙˆÙ†Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± timestamp: {combined_df['timestamp'].head()}")

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†ÙˆØ¹ datetime
        if not pd.api.types.is_datetime64_any_dtype(combined_df['timestamp']):
            logging.warning("ØªØ¨Ø¯ÛŒÙ„ Ù…Ø¬Ø¯Ø¯ timestamp Ø¨Ù‡ datetime...")
            combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], errors='coerce')
            combined_df = combined_df.dropna(subset=['timestamp'])
        
        # Debug timestamp Ù‚Ø¨Ù„ Ø§Ø² ØªØ¬Ù…ÛŒØ¹
        self.debug_timestamp_column(combined_df, "Ù‚Ø¨Ù„ Ø§Ø² ØªØ¬Ù…ÛŒØ¹")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± ØªØ¬Ù…ÛŒØ¹ÛŒ
        daily_stats, hourly_stats = self.aggregate_sentiments(combined_df)
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
        self.generate_sentiment_report(combined_df)
        
        self.sentiment_data = combined_df
        
        return combined_df, daily_stats, hourly_stats
    
    def aggregate_sentiments(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± ØªØ¬Ù…ÛŒØ¹ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª"""
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡
        if df.empty:
            logging.warning("DataFrame Ø®Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ø§Ø­Ø³Ø§Ø³Ø§Øª")
            return pd.DataFrame(), pd.DataFrame()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ø³ØªÙˆÙ† timestamp
        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
            logging.error(f"Ø®Ø·Ø§: Ø³ØªÙˆÙ† timestamp Ù†ÙˆØ¹ datetime Ù†Ø¯Ø§Ø±Ø¯. Ù†ÙˆØ¹ ÙØ¹Ù„ÛŒ: {df['timestamp'].dtype}")
            logging.error(f"Ù†Ù…ÙˆÙ†Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ±: {df['timestamp'].head()}")
            
            # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„
            try:
                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
                df = df.dropna(subset=['timestamp'])
                logging.info("âœ… timestamp Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯")
            except Exception as e:
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ timestamp: {e}")
                return pd.DataFrame(), pd.DataFrame()
        
        # Ø¢Ù…Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯
        try:
            df['date'] = df['timestamp'].dt.date
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ date Ø§Ø² timestamp: {e}")
            logging.error(f"Ù†ÙˆØ¹ timestamp: {df['timestamp'].dtype}")
            return pd.DataFrame(), pd.DataFrame()
        
        # Ø¢Ù…Ø§Ø± Ø§ÙˆÙ„ÛŒÙ‡
        agg_dict = {
            'sentiment_compound': ['mean', 'std', 'min', 'max'],
            'sentiment_positive': 'mean',
            'sentiment_negative': 'mean',
            'sentiment_neutral': 'mean',
            'text_length': 'mean'
        }
        
        # Ø§Ú¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ Reddit Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
        if 'reddit_score' in df.columns:
            agg_dict['reddit_score'] = 'mean'
        if 'reddit_comments' in df.columns:
            agg_dict['reddit_comments'] = 'mean'
        
        daily_stats = df.groupby(['symbol', 'date']).agg(agg_dict).round(4)
        
        # ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        daily_stats.columns = ['_'.join(col).strip() for col in daily_stats.columns.values]
        daily_stats = daily_stats.reset_index()
        
        # Ø§ÙØ²ÙˆØ¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ùˆ ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹
        news_count = df.groupby(['symbol', 'date']).size().reset_index(name='news_count')
        daily_stats = pd.merge(daily_stats, news_count, on=['symbol', 'date'], how='left')
        
        # ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù
        if 'detected_source' in df.columns:
            source_diversity = df.groupby(['symbol', 'date'])['detected_source'].nunique().reset_index(name='source_diversity')
            daily_stats = pd.merge(daily_stats, source_diversity, on=['symbol', 'date'], how='left')
        
        # Ø¢Ù…Ø§Ø± Ø³Ø§Ø¹ØªÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯
        try:
            df['hour'] = df['timestamp'].dt.floor('H')
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ hour: {e}")
            return daily_stats, pd.DataFrame()
        
        hourly_agg = {
            'sentiment_compound': ['mean', 'count'],
            'sentiment_positive': 'mean',
            'sentiment_negative': 'mean',
            'sentiment_neutral': 'mean'
        }
        
        # Ø§Ú¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ Reddit Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
        if 'reddit_score' in df.columns:
            hourly_agg['reddit_score'] = 'mean'
        
        hourly_stats = df.groupby(['symbol', 'hour']).agg(hourly_agg).round(4)
        
        hourly_stats.columns = ['_'.join(col).strip() for col in hourly_stats.columns.values]
        hourly_stats = hourly_stats.reset_index()
        
        return daily_stats, hourly_stats
    
    def generate_sentiment_report(self, df: pd.DataFrame):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø­Ø³Ø§Ø³Ø§Øª - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        logging.info("\n" + "="*60)
        logging.info("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Enhanced)")
        logging.info("="*60)
        
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        total_news = len(df)
        date_range = f"{df['timestamp'].min().date()} ØªØ§ {df['timestamp'].max().date()}"
        
        logging.info(f"ğŸ“… Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {date_range}")
        logging.info(f"ğŸ“° ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø±: {total_news:,}")
        logging.info(f"ğŸª™ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§: {df['symbol'].nunique()}")
        
        # Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹ (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
        if 'detected_source' in df.columns:
            source_dist = df['detected_source'].value_counts()
            logging.info(f"\nğŸ“¡ ØªÙˆØ²ÛŒØ¹ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ:")
            for source, count in source_dist.items():
                percentage = (count / total_news) * 100
                emoji = {'GNews': 'ğŸŒ', 'NewsAPI': 'ğŸ“°', 'CoinGecko': 'ğŸ¦', 'RSS': 'ğŸ“¡', 'Reddit': 'ğŸ”´'}.get(source, 'ğŸ“Š')
                logging.info(f"   {emoji} {source}: {count:,} ({percentage:.1f}%)")
        
        # ØªÙˆØ²ÛŒØ¹ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú©Ù„ÛŒ
        sentiment_dist = df['sentiment_label'].value_counts()
        logging.info("\nğŸ­ ØªÙˆØ²ÛŒØ¹ Ø§Ø­Ø³Ø§Ø³Ø§Øª:")
        for label, count in sentiment_dist.items():
            percentage = (count / total_news) * 100
            emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜Ÿ', 'neutral': 'ğŸ˜'}.get(label, '')
            logging.info(f"   {emoji} {label}: {count:,} ({percentage:.1f}%)")
        
        # Ø¢Ù…Ø§Ø± Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ù†Ù…Ø§Ø¯
        logging.info("\nğŸ“ˆ Ø¢Ù…Ø§Ø± Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ù†Ù…Ø§Ø¯:")
        symbol_stats = df.groupby('symbol').agg({
            'sentiment_compound': ['mean', 'std', 'count'],
            'sentiment_label': lambda x: (x == 'positive').sum() / len(x) * 100
        }).round(3)
        
        for symbol in symbol_stats.index[:10]:  # Ù†Ù…Ø§ÛŒØ´ 10 Ù†Ù…Ø§Ø¯ Ø¨Ø±ØªØ±
            mean_sentiment = symbol_stats.loc[symbol, ('sentiment_compound', 'mean')]
            std_sentiment = symbol_stats.loc[symbol, ('sentiment_compound', 'std')]
            count = symbol_stats.loc[symbol, ('sentiment_compound', 'count')]
            positive_pct = symbol_stats.loc[symbol, ('sentiment_label', '<lambda>')]
            
            emoji = 'ğŸŸ¢' if mean_sentiment > 0.1 else ('ğŸ”´' if mean_sentiment < -0.1 else 'ğŸŸ¡')
            logging.info(f"   {emoji} {symbol}: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={mean_sentiment:.3f}, "
                        f"Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={std_sentiment:.3f}, "
                        f"ØªØ¹Ø¯Ø§Ø¯={count}, Ù…Ø«Ø¨Øª={positive_pct:.1f}%")
        
        # Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ù‡ Reddit (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
        if 'reddit_score' in df.columns:
            reddit_df = df[df['detected_source'] == 'Reddit']
            if not reddit_df.empty:
                logging.info("\nğŸ”´ Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ù‡ Reddit:")
                avg_score = reddit_df['reddit_score'].mean()
                avg_comments = reddit_df['reddit_comments'].mean()
                logging.info(f"   Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø³Øªâ€ŒÙ‡Ø§: {avg_score:.1f}")
                logging.info(f"   Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§: {avg_comments:.1f}")
    
    def normalize_timezone(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ timezone Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ merge"""
        if df.empty:
            return df
        
        # Ø§Ú¯Ø± timestamp timezone-aware Ø§Ø³ØªØŒ Ø¨Ù‡ UTC ØªØ¨Ø¯ÛŒÙ„ Ú©Ù† Ùˆ Ø³Ù¾Ø³ timezone Ø±Ø§ Ø­Ø°Ù Ú©Ù†
        if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:
            df.index = df.index.tz_convert('UTC').tz_localize(None)
        
        # Ù‡Ù…ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ timestamp Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡
        for col in df.columns:
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                if df[col].dt.tz is not None:
                    df[col] = df[col].dt.tz_convert('UTC').dt.tz_localize(None)
        
        return df
    
    def merge_price_and_sentiment(self) -> pd.DataFrame:
        """
        Ø§Ø¯ØºØ§Ù… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¹Ø¯Ù… ØªØ·Ø¨ÛŒÙ‚ Ø²Ù…Ø§Ù†ÛŒ
        
        Ø±Ø§Ù‡â€ŒØ­Ù„: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Broadcasting Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¯ÙˆØ±Ù‡
        """
        logging.info("Ø´Ø±ÙˆØ¹ Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        
        if self.price_data is None or self.price_data.empty:
            logging.error("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
            return pd.DataFrame()
        
        if self.sentiment_data is None or self.sentiment_data.empty:
            logging.warning("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± ØµÙØ±
            price_data = self.price_data.reset_index()
            price_data['sentiment_compound_mean'] = 0
            price_data['sentiment_positive_mean'] = 0
            price_data['sentiment_negative_mean'] = 0
            price_data['sentiment_neutral_mean'] = 0
            price_data.set_index(['symbol', 'timeframe', 'timestamp'], inplace=True)
            return price_data
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ timezone
        logging.info("Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ timezone...")
        price_data = self.price_data.reset_index()
        price_data = self.normalize_timezone(price_data)
        
        sentiment_data = self.sentiment_data.copy()
        sentiment_data = self.normalize_timezone(sentiment_data)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú©Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯ (Ø¨Ø¯ÙˆÙ† ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø²Ù…Ø§Ù†)
        logging.info("Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú©Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯...")
        
        # Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡
        basic_agg = {
            'sentiment_compound': ['mean', 'std', 'count'],
            'sentiment_positive': 'mean',
            'sentiment_negative': 'mean',
            'sentiment_neutral': 'mean'
        }
        
        # Ø§Ú¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Reddit Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
        if 'reddit_score' in sentiment_data.columns:
            basic_agg['reddit_score'] = 'mean'
        if 'reddit_comments' in sentiment_data.columns:
            basic_agg['reddit_comments'] = 'mean'
        
        sentiment_symbol_stats = sentiment_data.groupby('symbol').agg(basic_agg).round(4)
        
        # ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        sentiment_symbol_stats.columns = ['_'.join(col).strip() for col in sentiment_symbol_stats.columns.values]
        sentiment_symbol_stats = sentiment_symbol_stats.reset_index()
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹
        if 'detected_source' in sentiment_data.columns:
            source_diversity = sentiment_data.groupby('symbol')['detected_source'].nunique().reset_index(name='source_diversity')
            sentiment_symbol_stats = pd.merge(sentiment_symbol_stats, source_diversity, on='symbol', how='left')
        
        # Ø§Ø¯ØºØ§Ù… Ú©Ù„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù…Ø§Ø¯ (Broadcast Ø§Ø­Ø³Ø§Ø³Ø§Øª)
        logging.info(f"Broadcasting Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø±Ø§ÛŒ {len(price_data)} Ø±Ú©ÙˆØ±Ø¯ Ù‚ÛŒÙ…Øª...")
        
        merged_data = pd.merge(
            price_data,
            sentiment_symbol_stats,
            on='symbol',
            how='left'
        )
        
        # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        sentiment_columns = [col for col in merged_data.columns if 'sentiment' in col or 'reddit' in col or 'source_diversity' in col]
        for col in sentiment_columns:
            merged_data[col] = merged_data[col].fillna(0)
        
        # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† index
        merged_data.set_index(['symbol', 'timeframe', 'timestamp'], inplace=True)
        merged_data.sort_index(inplace=True)
        
        logging.info(f"âœ… Ø§Ø¯ØºØ§Ù… ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡: {merged_data.shape}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡
        sentiment_cols = [col for col in merged_data.columns if 'sentiment' in col or 'reddit' in col]
        if sentiment_cols:
            logging.info(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡:")
            for col in sentiment_cols:
                non_zero = (merged_data[col] != 0).sum()
                mean_val = merged_data[col].mean()
                logging.info(f"   {col}: ØªØ¹Ø¯Ø§Ø¯ ØºÛŒØ± ØµÙØ± = {non_zero} ({non_zero/len(merged_data)*100:.1f}%), Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† = {mean_val:.4f}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹
        if 'source_diversity' in merged_data.columns:
            avg_diversity = merged_data['source_diversity'].mean()
            logging.info(f"   ğŸ“¡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹: {avg_diversity:.2f}")
        
        return merged_data
    
    def save_processed_data(self, price_df: pd.DataFrame, sentiment_raw: pd.DataFrame,
                          sentiment_daily: pd.DataFrame, sentiment_hourly: pd.DataFrame,
                          merged_df: pd.DataFrame) -> Dict[str, str]:
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡"""
        timestamp_str = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        saved_files = {}
        
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù‚ØµØ¯
        os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
        if not price_df.empty:
            price_filename = f'master_ohlcv_data_{timestamp_str}.parquet'
            price_path = os.path.join(PROCESSED_DATA_PATH, price_filename)            
            price_df.to_parquet(price_path)
            saved_files['price'] = price_path
            logging.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {price_path}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        if not sentiment_raw.empty:
            # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø­Ø³Ø§Ø³Ø§Øª
            raw_filename = f'sentiment_scores_raw_{timestamp_str}.parquet'
            raw_path = os.path.join(PROCESSED_DATA_PATH, raw_filename)
            sentiment_raw.to_parquet(raw_path, index=False)
            saved_files['sentiment_raw'] = raw_path
            
            # Ø¢Ù…Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡
            if not sentiment_daily.empty:
                daily_filename = f'sentiment_scores_daily_{timestamp_str}.parquet'
                daily_path = os.path.join(PROCESSED_DATA_PATH, daily_filename)
                sentiment_daily.to_parquet(daily_path, index=False)
                saved_files['sentiment_daily'] = daily_path
            
            # Ø¢Ù…Ø§Ø± Ø³Ø§Ø¹ØªÛŒ
            if not sentiment_hourly.empty:
                hourly_filename = f'sentiment_scores_hourly_{timestamp_str}.parquet'
                hourly_path = os.path.join(PROCESSED_DATA_PATH, hourly_filename)
                sentiment_hourly.to_parquet(hourly_path, index=False)
                saved_files['sentiment_hourly'] = hourly_path
            
            logging.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡
        if not merged_df.empty:
            merged_filename = f'master_merged_data_{timestamp_str}.parquet'
            merged_path = os.path.join(PROCESSED_DATA_PATH, merged_filename)
            merged_df.to_parquet(merged_path)
            saved_files['merged'] = merged_path
            logging.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {merged_path}")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆÙ†Ù‡ CSV Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ
            sample_csv = f'merged_sample_{timestamp_str}.csv'
            sample_path = os.path.join(PROCESSED_DATA_PATH, sample_csv)
            merged_df.head(1000).to_csv(sample_path, encoding='utf-8-sig')
            saved_files['sample'] = sample_path
        
        logging.info("\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡:")
        for file_type, path in saved_files.items():
            logging.info(f"   {file_type}: {path}")
        
        return saved_files

def run_unified_processing(process_price: bool = True, process_sentiment: bool = True,
                         merge_data: bool = True):
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
    logging.info("="*80)
    logging.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Enhanced Unified ETL)")
    logging.info("="*80)
    
    processor = UnifiedDataProcessor()
    
    # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
    price_df = pd.DataFrame()
    sentiment_raw = pd.DataFrame()
    sentiment_daily = pd.DataFrame()
    sentiment_hourly = pd.DataFrame()
    merged_df = pd.DataFrame()
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª
    if process_price:
        logging.info("\nğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 1: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª")
        price_df = processor.process_price_data()
        
        if not price_df.empty:
            logging.info(f"âœ… ØªØ¹Ø¯Ø§Ø¯ {len(price_df)} Ø±Ú©ÙˆØ±Ø¯ Ù‚ÛŒÙ…Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
            logging.info(f"ğŸ“ˆ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§: {price_df.index.get_level_values('symbol').nunique()}")
            logging.info(f"â±ï¸ ØªØ¹Ø¯Ø§Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§: {price_df.index.get_level_values('timeframe').nunique()}")
        else:
            logging.warning("âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ù‚ÛŒÙ…ØªÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯")
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
    if process_sentiment:
        logging.info("\nğŸ­ Ù…Ø±Ø­Ù„Ù‡ 2: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Enhanced)")
        sentiment_raw, sentiment_daily, sentiment_hourly = processor.process_sentiment_data()
        
        if not sentiment_raw.empty:
            logging.info(f"âœ… ØªØ¹Ø¯Ø§Ø¯ {len(sentiment_raw)} Ø®Ø¨Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
            logging.info(f"ğŸ“° ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§: {sentiment_raw['symbol'].nunique()}")
            
            # Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹
            if 'detected_source' in sentiment_raw.columns:
                source_counts = sentiment_raw['detected_source'].value_counts()
                logging.info(f"ğŸ“¡ Ù…Ù†Ø§Ø¨Ø¹ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {dict(source_counts)}")
        else:
            logging.warning("âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯")
    
    # Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    if merge_data and not price_df.empty:
        logging.info("\nğŸ”— Ù…Ø±Ø­Ù„Ù‡ 3: Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Enhanced Broadcasting)")
        merged_df = processor.merge_price_and_sentiment()
        
        if not merged_df.empty:
            logging.info(f"âœ… Ø§Ø¯ØºØ§Ù… Ù…ÙˆÙÙ‚: {merged_df.shape}")
            
            # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
            sentiment_cols = [col for col in merged_df.columns if 'sentiment' in col or 'reddit' in col]
            if sentiment_cols:
                logging.info(f"ğŸ­ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡: {len(sentiment_cols)} Ø³ØªÙˆÙ†")
                for col in sentiment_cols[:5]:  # Ù†Ù…Ø§ÛŒØ´ 5 Ø³ØªÙˆÙ† Ø§ÙˆÙ„
                    logging.info(f"     - {col}")
                if len(sentiment_cols) > 5:
                    logging.info(f"     ... Ùˆ {len(sentiment_cols) - 5} Ø³ØªÙˆÙ† Ø¯ÛŒÚ¯Ø±")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    logging.info("\nğŸ’¾ Ù…Ø±Ø­Ù„Ù‡ 4: Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡")
    saved_files = processor.save_processed_data(
        price_df, sentiment_raw, sentiment_daily, sentiment_hourly, merged_df
    )
    
    # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n" + "="*80)
    print("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ (Enhanced)")
    print("="*80)
    
    if process_price:
        print(f"\nğŸ’° Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª:")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯: {len(price_df):,}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯: {price_df.index.get_level_values('symbol').nunique() if not price_df.empty else 0}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…: {price_df.index.get_level_values('timeframe').nunique() if not price_df.empty else 0}")
    
    if process_sentiment:
        print(f"\nğŸ­ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª:")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ø®Ø¨Ø±: {len(sentiment_raw):,}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯: {sentiment_raw['symbol'].nunique() if not sentiment_raw.empty else 0}")
        
        # Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹
        if not sentiment_raw.empty and 'detected_source' in sentiment_raw.columns:
            source_counts = sentiment_raw['detected_source'].value_counts()
            print(f"   - Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ:")
            for source, count in source_counts.items():
                print(f"     ğŸ“¡ {source}: {count:,} Ø®Ø¨Ø±")
    
    if merge_data and not merged_df.empty:
        print(f"\nğŸ”— Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡:")
        print(f"   - Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ: {merged_df.shape}")
        sentiment_features = [col for col in merged_df.columns if 'sentiment' in col or 'reddit' in col]
        print(f"   - ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª: {len(sentiment_features)}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±ØµØ¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø§Ø±Ù†Ø¯
        non_zero_sentiment = 0
        if sentiment_features:
            for col in sentiment_features:
                if 'compound' in col and 'mean' in col:
                    non_zero_sentiment = (merged_df[col] != 0).sum()
                    mean_val = merged_df[col].mean()
                    print(f"   - Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª: {non_zero_sentiment:,} ({non_zero_sentiment/len(merged_df)*100:.1f}%)")
                    print(f"   - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø­Ø³Ø§Ø³Ø§Øª: {mean_val:.4f}")
                    break
        
        # Ø¢Ù…Ø§Ø± Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡
        if 'source_diversity' in merged_df.columns:
            avg_diversity = merged_df['source_diversity'].mean()
            max_diversity = merged_df['source_diversity'].max()
            print(f"   - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹: {avg_diversity:.2f}")
            print(f"   - Ø­Ø¯Ø§Ú©Ø«Ø± ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹: {max_diversity:.0f}")
    
    print("\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡:")
    for file_type, path in saved_files.items():
        print(f"   - {file_type}: {os.path.basename(path)}")
    
    print("="*80)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡
    if not merged_df.empty:
        print("\n--- Ù†Ù…ÙˆÙ†Ù‡ 5 Ø±Ø¯ÛŒÙ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡ (Enhanced) ---")
        display_cols = ['open', 'high', 'low', 'close', 'volume']
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…Ù‡Ù…
        sentiment_display_cols = []
        for col in merged_df.columns:
            if 'sentiment_compound_mean' in col or 'source_diversity' in col:
                sentiment_display_cols.append(col)
            if len(sentiment_display_cols) >= 3:  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ø³ØªÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§Øª
                break
        
        display_cols.extend(sentiment_display_cols)
        print(merged_df[display_cols].head())

def get_user_options():
    """Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² Ú©Ø§Ø±Ø¨Ø±"""
    print("\n" + "="*60)
    print("âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ (Enhanced - Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ fetch_01_fixed)")
    print("="*60)
    
    print("\nØ§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ Ú†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆÙ†Ø¯:")
    print("1. ÙÙ‚Ø· Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª")
    print("2. ÙÙ‚Ø· Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª")
    print("3. Ù‡Ø± Ø¯Ùˆ (Ù‚ÛŒÙ…Øª Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª) - Ø¨Ø¯ÙˆÙ† Ø§Ø¯ØºØ§Ù…")
    print("4. Ù‡Ø± Ø¯Ùˆ + Ø§Ø¯ØºØ§Ù… (ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)")
    
    choice = input("\nØ§Ù†ØªØ®Ø§Ø¨ Ø´Ù…Ø§ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 4): ").strip() or '4'
    
    process_price = choice in ['1', '3', '4']
    process_sentiment = choice in ['2', '3', '4']
    merge_data = choice == '4'
    
    print("\nğŸ”§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø³Ø®Ù‡ Enhanced:")
    print("âœ… Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø§ fetch_01_fixed_clean.py")
    print("âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯: Reddit, NewsAPI, RSS, CoinGecko")
    print("âœ… ØªØ´Ø®ÛŒØµ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ùˆ Ø®Ø¨Ø±ÛŒ")
    print("âœ… Broadcasting Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø´Ú©Ù„ Ø¹Ø¯Ù… ØªØ·Ø¨ÛŒÙ‚ Ø²Ù…Ø§Ù†ÛŒ")
    print("âœ… Ø¢Ù…Ø§Ø± ØªÙØµÛŒÙ„ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ")
    
    return process_price, process_sentiment, merge_data

if __name__ == '__main__':
    # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² Ú©Ø§Ø±Ø¨Ø±
    process_price, process_sentiment, merge_data = get_user_options()
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
    run_unified_processing(process_price, process_sentiment, merge_data)