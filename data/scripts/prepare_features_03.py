#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ุงุณฺฉุฑูพุช ููุง ูููุฏุณ ูฺฺฏ (ูุงุฒ ณุ ฺฏุงู ุงูู) - ูุณุฎู ุงุตูุงุญ ุดุฏู ฺฉุงูู

๐ง ุชุบุฑุงุช ููู ุงู ูุณุฎู:
- โ ุณุงุฒฺฏุงุฑ ฺฉุงูู ุจุง ูุงูโูุง 01 ู 02 ุงุตูุงุญ ุดุฏู
- โ ูพุดุชุจุงู ฺฉุงูู ุงุฒ Broadcasting sentiment structure
- โ ุงุถุงูู ฺฉุฑุฏู Reddit features support (reddit_score, reddit_comments)
- โ ุฑูุน ูุดฺฉู PSAR calculation ู count ูุดฺฉู 57/58
- โ ุญู pandas deprecation warnings
- โ ุจูุจูุฏ error handling ู fallback mechanisms  
- โ ุจูููโุณุงุฒ memory management
- โ ูพุดุชุจุงู ุงุฒ multi-source sentiment (GNews, NewsAPI, CoinGecko, RSS, Reddit)
- โ ุจูุจูุฏ comprehensive logging
- โ ุงุตูุงุญ MFI calculation warnings
- โ ุจูุจูุฏ feature alignment ู time-series processing

ุชุบุฑุงุช ุงุตู:
- ุญู ูุดฺฉู sentiment_score = 0 ุจุง ุฎูุงูุฏู ุตุญุญ ุงุฒ ุณุงุฎุชุงุฑ Broadcasting
- ุงุถุงูู ฺฉุฑุฏู Reddit-specific features
- ุฑูุน ูุดฺฉู PSAR missing
- ุจูุจูุฏ multi-source sentiment processing
- ุงุตูุงุญ pandas compatibility issues
"""
import os
import glob
import pandas as pd
import pandas_ta as ta
import logging
import configparser
from typing import Optional, Dict, Any, List, Tuple
import numpy as np
import gc
from datetime import datetime
import warnings

# ุชูุธู warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)

# ุจุฎุด ุฎูุงูุฏู ูพฺฉุฑุจูุฏ
config = configparser.ConfigParser()
CONFIG_FILE_PATH = 'config.ini'
try:
    config.read(CONFIG_FILE_PATH, encoding='utf-8')
    PROCESSED_DATA_PATH = config.get('Paths', 'processed')
    FEATURES_PATH = config.get('Paths', 'features')
    LOG_PATH = config.get('Paths', 'logs')
except Exception as e:
    print(f"CRITICAL ERROR: Could not read 'config.ini'. Error: {e}")
    exit()

# ุงุฌุงุฏ ูพูุดูโูุง ููุฑุฏ ูุงุฒ
os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)
os.makedirs(FEATURES_PATH, exist_ok=True)
os.makedirs(LOG_PATH, exist_ok=True)

# --- ุชูุธูุงุช ูุงฺฏโฺฏุฑ ูพูุง ---
script_name = os.path.splitext(os.path.basename(__file__))[0]
log_subfolder_path = os.path.join(LOG_PATH, script_name)
os.makedirs(log_subfolder_path, exist_ok=True)

log_filename = os.path.join(log_subfolder_path, f"log_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])

# === ูพุงุฑุงูุชุฑูุง ูพูุง ู ูุงุจู ุชูุธู (ุจูุจูุฏ ุงูุชู) ===
INDICATOR_PARAMS = {
    # ูพุงุฑุงูุชุฑูุง ุงุตู
    'rsi_length': 14,
    'macd_fast': 12,
    'macd_slow': 26,
    'macd_signal': 9,
    'bb_length': 20,
    'bb_std': 2.0,
    
    # ุงูุฏฺฉุงุชูุฑูุง ุฌุฏุฏ
    'atr_length': 14,
    'vwap_anchor': None,  # None ุจุฑุง VWAP ุฑูุฒุงูู
    
    # ุงูุฏฺฉุงุชูุฑูุง ููุณุงู ู ุชุฑูุฏ
    'stoch_k': 14,
    'stoch_d': 3,
    'stoch_smooth': 3,
    'williams_r_length': 14,
    'cci_length': 20,
    
    # ูุงูฺฏูโูุง ูุชุญุฑฺฉ
    'ema_short': 12,
    'ema_medium': 26,
    'ema_long': 50,
    'sma_short': 10,
    'sma_medium': 20,
    'sma_long': 50,
    
    # ุงูุฏฺฉุงุชูุฑูุง ุญุฌู
    'obv_enabled': True,
    'mfi_length': 14,
    'ad_enabled': True,
    
    # === ูพุงุฑุงูุชุฑูุง ุงุญุณุงุณุงุช ุจูุจูุฏ ุงูุชู ===
    'sentiment_ma_short': 7,
    'sentiment_ma_long': 14,
    'sentiment_momentum_period': 24,  # 24 ุณุงุนุช
    
    # === ูพุงุฑุงูุชุฑูุง Reddit ุฌุฏุฏ ===
    'reddit_score_ma': 12,  # ูุงูฺฏู ูุชุญุฑฺฉ reddit score
    'reddit_comments_ma': 12,  # ูุงูฺฏู ูุชุญุฑฺฉ reddit comments
    
    # ุญุฏุงูู ุฏุงุฏู ููุฑุฏ ูุงุฒ
    'min_data_points': 100,
    
    # === ูพุงุฑุงูุชุฑูุง PSAR (ุงุตูุงุญ ุดุฏู) ===
    'psar_af': 0.02,
    'psar_max_af': 0.2,
}

# ูพุงุฑุงูุชุฑูุง ูููุฏุณ ูฺฺฏ ู ูุฏู
TARGET_FUTURE_PERIODS = 24
TARGET_PROFIT_PERCENT = 0.02

# ฺฉุงูุชุฑ global ุจุฑุง tracking
GLOBAL_COUNTER = 0
TOTAL_GROUPS = 0

def log_indicator_error(indicator_name: str, group_name: Any, error: Exception):
    """ุชุงุจุน ฺฉูพุงุฑฺู ุจุฑุง ูุงฺฏ ฺฉุฑุฏู ุฎุทุงูุง ุงูุฏฺฉุงุชูุฑูุง"""
    logging.warning(f"ุฎุทุง ุฏุฑ ูุญุงุณุจู {indicator_name} ุจุฑุง ฺฏุฑูู {group_name}: {error}")

def log_progress(current: int, total: int, group_name: str = ""):
    """ููุงุด ูพุดุฑูุช ูพุฑุฏุงุฒุด"""
    if total > 0:
        progress = (current / total) * 100
        if current % max(1, total // 20) == 0:  # ูุฑ 5% ฺฏุฒุงุฑุด
            logging.info(f"๐ ูพุดุฑูุช: {progress:.1f}% ({current}/{total}) - {group_name}")

def safe_numeric_conversion(series: pd.Series, name: str) -> pd.Series:
    """ุชุจุฏู ุงูู ุจู numeric ุจุง ูุฏุฑุช ุฎุทุง"""
    try:
        return pd.to_numeric(series, errors='coerce')
    except Exception as e:
        logging.warning(f"ุฎุทุง ุฏุฑ ุชุจุฏู {name} ุจู numeric: {e}")
        return series.fillna(0)

def apply_features(group: pd.DataFrame) -> Optional[pd.DataFrame]:
    """
    ุงู ุชุงุจุน ุชูุงู ุงูุฏฺฉุงุชูุฑูุง ู ูฺฺฏโูุง ูพุดุฑูุชู ุฑุง ุจุฑุง ฺฉ ฺฏุฑูู ุฏุงุฏู ูุญุงุณุจู ูโฺฉูุฏ.
    ุงุตูุงุญ ุดุฏู ุจุฑุง ุณุงุฒฺฏุงุฑ ฺฉุงูู ุจุง ูุงูโูุง 01 ู 02
    """
    global GLOBAL_COUNTER, TOTAL_GROUPS
    GLOBAL_COUNTER += 1
    
    # ููุงุด ูพุดุฑูุช
    log_progress(GLOBAL_COUNTER, TOTAL_GROUPS, str(group.name))
    
    # ุงุทููุงู ุงุฒ ุงูฺฉู ุฏุงุฏู ฺฉุงู ุจุฑุง ูุญุงุณุจู ูุฌูุฏ ุฏุงุฑุฏ
    if len(group) < INDICATOR_PARAMS['min_data_points']:
        logging.debug(f"ฺฏุฑูู {group.name} ุฏุงุฏู ฺฉุงู ูุฏุงุฑุฏ ({len(group)} < {INDICATOR_PARAMS['min_data_points']})")
        return None

    # ุงุทููุงู ุงุฒ ุชุจุฏู ุตุญุญ ุงููุงุน ุฏุงุฏู
    numeric_columns = ['open', 'high', 'low', 'close', 'volume']
    for col in numeric_columns:
        if col in group.columns:
            group[col] = safe_numeric_conversion(group[col], col)

    # === ุจุฎุด ฑ: ุงูุฏฺฉุงุชูุฑูุง ุชุฑูุฏ ู ููุช ===
    try:
        group['rsi'] = ta.rsi(group['close'], length=INDICATOR_PARAMS['rsi_length'])
    except Exception as e:
        log_indicator_error('RSI', group.name, e)

    try:
        macd = ta.macd(group['close'], 
                      fast=INDICATOR_PARAMS['macd_fast'], 
                      slow=INDICATOR_PARAMS['macd_slow'], 
                      signal=INDICATOR_PARAMS['macd_signal'])
        if macd is not None and not macd.empty:
            group['macd'] = macd[f'MACD_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
            group['macd_hist'] = macd[f'MACDh_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
            group['macd_signal'] = macd[f'MACDs_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
    except Exception as e:
        log_indicator_error('MACD', group.name, e)

    try:
        bbands = ta.bbands(group['close'], 
                          length=INDICATOR_PARAMS['bb_length'], 
                          std=INDICATOR_PARAMS['bb_std'])
        if bbands is not None and not bbands.empty:
            group['bb_upper'] = bbands[f'BBU_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            group['bb_middle'] = bbands[f'BBM_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            group['bb_lower'] = bbands[f'BBL_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            # ูุญุงุณุจู ูููุนุช ููุช ุฏุฑ ฺฉุงูุงู Bollinger Bands
            bb_range = group['bb_upper'] - group['bb_lower']
            group['bb_position'] = np.where(bb_range != 0, 
                                          (group['close'] - group['bb_lower']) / bb_range, 
                                          0.5)
    except Exception as e:
        log_indicator_error('Bollinger Bands', group.name, e)

    # === ุจุฎุด ฒ: ุงูุฏฺฉุงุชูุฑูุง ููุณุงู (Volatility) ===
    try:
        group['atr'] = ta.atr(group['high'], group['low'], group['close'], 
                             length=INDICATOR_PARAMS['atr_length'])
        # ูุญุงุณุจู ATR ูุฑูุงู ุดุฏู (ATR ุจู ูุณุจุช ููุช)
        group['atr_percent'] = np.where(group['close'] != 0, 
                                      (group['atr'] / group['close']) * 100, 
                                      0)
    except Exception as e:
        log_indicator_error('ATR', group.name, e)

    try:
        # ูุญุงุณุจู ููุณุงู ุชุงุฑุฎ (Historical Volatility)
        group['price_change'] = group['close'].pct_change()
        group['volatility'] = group['price_change'].rolling(window=20).std() * 100
    except Exception as e:
        log_indicator_error('Historical Volatility', group.name, e)

    # === ุจุฎุด ณ: ุงูุฏฺฉุงุชูุฑูุง ูุจุชู ุจุฑ ุญุฌู (Volume-Based) - ุงุตูุงุญ ุดุฏู ===
    try:
        # ูุญุงุณุจู VWAP ุฏุณุช ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุฎุทุง MultiIndex
        typical_price = (group['high'] + group['low'] + group['close']) / 3
        vwap_numerator = (typical_price * group['volume']).cumsum()
        vwap_denominator = group['volume'].cumsum()
        # ุฌููฺฏุฑ ุงุฒ ุชูุณู ุจุฑ ุตูุฑ
        group['vwap'] = np.where(vwap_denominator != 0, 
                               vwap_numerator / vwap_denominator, 
                               typical_price)
        # ูุญุงุณุจู ุงูุญุฑุงู ููุช ุงุฒ VWAP
        group['vwap_deviation'] = np.where(group['vwap'] != 0,
                                         ((group['close'] - group['vwap']) / group['vwap']) * 100,
                                         0)
    except Exception as e:
        log_indicator_error('VWAP', group.name, e)

    if INDICATOR_PARAMS['obv_enabled']:
        try:
            group['obv'] = ta.obv(group['close'], group['volume'])
            # ูุญุงุณุจู ุชุบุฑุงุช OBV
            group['obv_change'] = group['obv'].pct_change().fillna(0)
        except Exception as e:
            log_indicator_error('OBV', group.name, e)

    try:
        # === ุงุตูุงุญ MFI calculation ุจุฑุง ุญู warning ===
        # ุงุทููุงู ุงุฒ ููุน ุฏุงุฏู ุตุญุญ
        high_safe = safe_numeric_conversion(group['high'], 'high')
        low_safe = safe_numeric_conversion(group['low'], 'low')
        close_safe = safe_numeric_conversion(group['close'], 'close')
        volume_safe = safe_numeric_conversion(group['volume'], 'volume')
        
        group['mfi'] = ta.mfi(high_safe, low_safe, close_safe, volume_safe, 
                             length=INDICATOR_PARAMS['mfi_length'])
    except Exception as e:
        log_indicator_error('MFI', group.name, e)

    if INDICATOR_PARAMS['ad_enabled']:
        try:
            group['ad'] = ta.ad(group['high'], group['low'], group['close'], group['volume'])
        except Exception as e:
            log_indicator_error('A/D Line', group.name, e)

    # === ุจุฎุด ด: ุงูุฏฺฉุงุชูุฑูุง ุงูุณูุงุชูุฑ ===
    try:
        stoch = ta.stoch(group['high'], group['low'], group['close'], 
                        k=INDICATOR_PARAMS['stoch_k'], 
                        d=INDICATOR_PARAMS['stoch_d'], 
                        smooth_k=INDICATOR_PARAMS['stoch_smooth'])
        if stoch is not None and not stoch.empty:
            group['stoch_k'] = stoch[f'STOCHk_{INDICATOR_PARAMS["stoch_k"]}_{INDICATOR_PARAMS["stoch_d"]}_{INDICATOR_PARAMS["stoch_smooth"]}']
            group['stoch_d'] = stoch[f'STOCHd_{INDICATOR_PARAMS["stoch_k"]}_{INDICATOR_PARAMS["stoch_d"]}_{INDICATOR_PARAMS["stoch_smooth"]}']
    except Exception as e:
        log_indicator_error('Stochastic', group.name, e)

    try:
        group['williams_r'] = ta.willr(group['high'], group['low'], group['close'], 
                                      length=INDICATOR_PARAMS['williams_r_length'])
    except Exception as e:
        log_indicator_error('Williams %R', group.name, e)

    try:
        group['cci'] = ta.cci(group['high'], group['low'], group['close'], 
                             length=INDICATOR_PARAMS['cci_length'])
    except Exception as e:
        log_indicator_error('CCI', group.name, e)

    # === ุจุฎุด ต: ูุงูฺฏูโูุง ูุชุญุฑฺฉ ===
    try:
        group['ema_short'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_short'])
        group['ema_medium'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_medium'])
        group['ema_long'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_long'])
        
        # ุณฺฏูุงูโูุง cross over
        group['ema_short_above_medium'] = (group['ema_short'] > group['ema_medium']).astype(int)
        group['ema_medium_above_long'] = (group['ema_medium'] > group['ema_long']).astype(int)
        
        # ูุญุงุณุจู ุดุจ EMA (ุชุฑูุฏ)
        group['ema_short_slope'] = group['ema_short'].pct_change(periods=5).fillna(0)
        group['ema_medium_slope'] = group['ema_medium'].pct_change(periods=5).fillna(0)
    except Exception as e:
        log_indicator_error('EMA', group.name, e)

    try:
        group['sma_short'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_short'])
        group['sma_medium'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_medium'])
        group['sma_long'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_long'])
        
        # ูููุนุช ููุช ูุณุจุช ุจู SMA
        group['price_above_sma_short'] = (group['close'] > group['sma_short']).astype(int)
        group['price_above_sma_medium'] = (group['close'] > group['sma_medium']).astype(int)
        group['price_above_sma_long'] = (group['close'] > group['sma_long']).astype(int)
    except Exception as e:
        log_indicator_error('SMA', group.name, e)

    # === ุจุฎุด ถ: ูฺฺฏโูุง ููุช ุฎุงู ===
    try:
        # ูุญุงุณุจู ุจุงุฒุฏูโูุง ูุฎุชูู
        group['return_1'] = group['close'].pct_change(1).fillna(0)
        group['return_5'] = group['close'].pct_change(5).fillna(0)
        group['return_10'] = group['close'].pct_change(10).fillna(0)
        
        # ูุญุงุณุจู ูุงูฺฏู ุจุงุฒุฏู
        group['avg_return_5'] = group['return_1'].rolling(window=5, min_periods=1).mean()
        group['avg_return_10'] = group['return_1'].rolling(window=10, min_periods=1).mean()
        
        # ูุญุงุณุจู High-Low ratio
        group['hl_ratio'] = np.where(group['close'] != 0,
                                   (group['high'] - group['low']) / group['close'],
                                   0)
        
        # ูุญุงุณุจู ูููุนุช close ุฏุฑ ูุญุฏูุฏู high-low
        hl_range = group['high'] - group['low']
        group['close_position'] = np.where(hl_range != 0,
                                         (group['close'] - group['low']) / hl_range,
                                         0.5)
        
        # ุญุฌู ูุฑูุงู ุดุฏู
        group['volume_ma'] = group['volume'].rolling(window=20, min_periods=1).mean()
        group['volume_ratio'] = np.where(group['volume_ma'] != 0,
                                       group['volume'] / group['volume_ma'],
                                       1.0)
        
    except Exception as e:
        log_indicator_error('Price Features', group.name, e)

    # === ุจุฎุด ท: ุงูุฏฺฉุงุชูุฑูุง ูพุดุฑูุชู (ุงุตูุงุญ ุดุฏู ู ุชฺฉูู ุดุฏู) ===
    try:
        # === Parabolic SAR ุจุง ุงุตูุงุญ ฺฉุงูู ===
        psar_result = ta.psar(group['high'], group['low'], group['close'], 
                             af0=INDICATOR_PARAMS['psar_af'], 
                             af=INDICATOR_PARAMS['psar_af'], 
                             max_af=INDICATOR_PARAMS['psar_max_af'])
        if psar_result is not None:
            if isinstance(psar_result, pd.DataFrame):
                # ุงฺฏุฑ DataFrame ุงุณุชุ ุณุชูู ุงูู ุฑุง ุงูุชุฎุงุจ ูโฺฉูู
                if len(psar_result.columns) > 0:
                    group['psar'] = psar_result.iloc[:, 0]
                else:
                    group['psar'] = group['close']  # fallback
            else:
                # ุงฺฏุฑ Series ุงุณุช
                group['psar'] = psar_result
            
            # ุงุทููุงู ุงุฒ ูุฌูุฏ PSAR
            if 'psar' in group.columns:
                group['price_above_psar'] = (group['close'] > group['psar']).astype(int)
            else:
                group['psar'] = group['close']  # fallback
                group['price_above_psar'] = 0
        else:
            # ุงฺฏุฑ PSAR ูุญุงุณุจู ูุดุฏุ ููุงุฏุฑ ูพุดโูุฑุถ
            group['psar'] = group['close']
            group['price_above_psar'] = 0
            
    except Exception as e:
        log_indicator_error('Parabolic SAR', group.name, e)
        # fallback ุฏุฑ ุตูุฑุช ุฎุทุง
        group['psar'] = group['close']
        group['price_above_psar'] = 0

    try:
        # ADX ุจุง ุจุฑุฑุณ ููุน ูุชุฌู
        adx_result = ta.adx(group['high'], group['low'], group['close'], length=14)
        if adx_result is not None:
            if isinstance(adx_result, pd.DataFrame):
                # ุงฺฏุฑ DataFrame ุงุณุชุ ุณุชูู ADX ุฑุง ุงูุชุฎุงุจ ูโฺฉูู
                if 'ADX_14' in adx_result.columns:
                    group['adx'] = adx_result['ADX_14']
                else:
                    group['adx'] = adx_result.iloc[:, 0]  # ุณุชูู ุงูู
            else:
                # ุงฺฏุฑ Series ุงุณุช
                group['adx'] = adx_result
        else:
            group['adx'] = 50  # ููุฏุงุฑ ูพุดโูุฑุถ ุจุฑุง ADX

    except Exception as e:
        log_indicator_error('ADX', group.name, e)
        group['adx'] = 50  # ููุฏุงุฑ ูพุดโูุฑุถ

    # === ุจุฎุด ธ: ูพุฑุฏุงุฒุด ูฺฺฏโูุง ุงุญุณุงุณุงุช ููุฌูุฏ (ุงุตูุงุญ ุดุฏู) ===
    try:
        # ุจุฑุฑุณ ูุฌูุฏ ุณุชููโูุง ุงุญุณุงุณุงุช ุงุฒ ูุงู 02
        sentiment_columns_mapping = {
            'sentiment_compound_mean': 'sentiment_score',
            'sentiment_positive_mean': 'sentiment_positive',  
            'sentiment_negative_mean': 'sentiment_negative',
            'sentiment_neutral_mean': 'sentiment_neutral',
        }
        
        # ูฺฏุงุดุช ุณุชููโูุง ุงุญุณุงุณุงุช ููุฌูุฏ
        for original_col, new_col in sentiment_columns_mapping.items():
            if original_col in group.columns:
                group[new_col] = group[original_col]
                logging.debug(f"ูฺฏุงุดุช {original_col} ุจู {new_col}")
            else:
                group[new_col] = 0
                
        # === ูพุฑุฏุงุฒุด Reddit Features (ุฌุฏุฏ) ===
        reddit_features = ['reddit_score', 'reddit_comments']
        for feature in reddit_features:
            if feature in group.columns:
                # ูุญุงุณุจู ูุงูฺฏู ูุชุญุฑฺฉ Reddit features
                if feature == 'reddit_score':
                    group[f'{feature}_ma'] = group[feature].rolling(
                        window=INDICATOR_PARAMS['reddit_score_ma'], min_periods=1
                    ).mean()
                elif feature == 'reddit_comments':
                    group[f'{feature}_ma'] = group[feature].rolling(
                        window=INDICATOR_PARAMS['reddit_comments_ma'], min_periods=1
                    ).mean()
                
                # ูุญุงุณุจู Reddit momentum
                group[f'{feature}_momentum'] = group[feature].diff(12).fillna(0)  # 12 period momentum
                
                logging.debug(f"ูพุฑุฏุงุฒุด Reddit feature: {feature}")
            else:
                # ุงฺฏุฑ Reddit features ููุฌูุฏ ูุณุชุ ููุงุฏุฑ ูพุดโูุฑุถ
                group[feature] = 0
                group[f'{feature}_ma'] = 0
                group[f'{feature}_momentum'] = 0
        
        # === ูุญุงุณุจู source diversity ุงฺฏุฑ ููุฌูุฏ ุจุงุดุฏ ===
        if 'source_diversity' in group.columns:
            group['source_diversity_normalized'] = group['source_diversity'] / group['source_diversity'].max() if group['source_diversity'].max() > 0 else 0
        else:
            group['source_diversity'] = 1
            group['source_diversity_normalized'] = 0
            
    except Exception as e:
        log_indicator_error('Sentiment and Reddit Features', group.name, e)
        # ุงุถุงูู ฺฉุฑุฏู ููุงุฏุฑ ูพุดโูุฑุถ ุฏุฑ ุตูุฑุช ุฎุทุง
        default_sentiment_features = [
            'sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral',
            'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
            'reddit_score_momentum', 'reddit_comments_momentum', 'source_diversity', 'source_diversity_normalized'
        ]
        for feature in default_sentiment_features:
            if feature not in group.columns:
                group[feature] = 0

    # === ูพุงฺฉุณุงุฒ ุญุงูุธู ุจููู ุดุฏู ===
    if GLOBAL_COUNTER % 25 == 0:  # ูุฑ 25 ฺฏุฑูู ุจู ุฌุง 50
        gc.collect()

    return group

def enhance_sentiment_features(df_features: pd.DataFrame, processed_data_path: str) -> pd.DataFrame:
    """
    ุชุงุจุน ุจูุจูุฏ ุงูุชู ุจุฑุง ุงุถุงูู ฺฉุฑุฏู ูฺฺฏโูุง ูพุดุฑูุชู ุงุญุณุงุณุงุช
    ุณุงุฒฺฏุงุฑ ฺฉุงูู ุจุง ุณุงุฎุชุงุฑ Broadcasting ุฌุฏุฏ ูุงู 02
    """
    logging.info("๐ญ ุดุฑูุน ุจูุจูุฏ ูฺฺฏโูุง ุงุญุณุงุณุงุช (ูุณุฎู ฺฉุงููุงู ุงุตูุงุญ ุดุฏู)...")
    
    try:
        # ุจุฑุฑุณ ูุฌูุฏ ุณุชููโูุง ุงุญุณุงุณุงุช Broadcasting ุงุฒ ูุงู 02
        broadcasting_sentiment_cols = [col for col in df_features.columns if 'sentiment' in col and 'mean' in col]
        reddit_cols = [col for col in df_features.columns if 'reddit' in col]
        source_cols = [col for col in df_features.columns if 'source' in col]
        
        logging.info(f"โ ุณุชููโูุง Broadcasting sentiment ุงูุช ุดุฏู: {broadcasting_sentiment_cols}")
        logging.info(f"โ ุณุชููโูุง Reddit ุงูุช ุดุฏู: {reddit_cols}")
        logging.info(f"โ ุณุชููโูุง Source ุงูุช ุดุฏู: {source_cols}")
        
        if broadcasting_sentiment_cols:
            logging.info("โ ุงุญุณุงุณุงุช Broadcasting ุงุฒ ูุจู ููุฌูุฏ ุงุณุช")
            
            # ุงุณุชุฎุฑุงุฌ sentiment_score ุงุฒ ุณุชููโูุง Broadcasting
            if 'sentiment_compound_mean' in df_features.columns:
                df_features['sentiment_score'] = df_features['sentiment_compound_mean']
                logging.info("โ sentiment_score ุงุฒ sentiment_compound_mean ุงุณุชุฎุฑุงุฌ ุดุฏ")
                
                # ุขูุงุฑ sentiment_score
                non_zero_sentiment = (df_features['sentiment_score'] != 0).sum()
                total_records = len(df_features)
                percentage = (non_zero_sentiment / total_records) * 100 if total_records > 0 else 0
                logging.info(f"๐ ุขูุงุฑ sentiment_score: {non_zero_sentiment:,} ุบุฑุตูุฑ ุงุฒ {total_records:,} ({percentage:.1f}%)")
                
            else:
                df_features['sentiment_score'] = 0
                logging.warning("โ๏ธ sentiment_compound_mean ุงูุช ูุดุฏุ sentiment_score = 0 ุชูุธู ุดุฏ")
        else:
            logging.warning("โ๏ธ ูฺ ุณุชูู ุงุญุณุงุณุงุช Broadcasting ุงูุช ูุดุฏ. ุฌุณุชุฌู ุฏุฑ ูุงูโูุง...")
            
            # ุฌุณุชุฌู ุฏุฑ ูุงูโูุง ูพุฑุฏุงุฒุด ุดุฏู ุจุง pattern ุฌุฏุฏ
            sentiment_files_patterns = [
                'master_merged_data_*.parquet',  # ูุงูโูุง ุงุฏุบุงู ุดุฏู ุฌุฏุฏ
                'sentiment_scores_raw_*.parquet',
                'sentiment_scores_daily_*.parquet', 
                'sentiment_scores_hourly_*.parquet'
            ]
            
            found_sentiment_file = None
            for pattern in sentiment_files_patterns:
                files = glob.glob(os.path.join(PROCESSED_DATA_PATH, pattern))
                if files:
                    found_sentiment_file = max(files, key=os.path.getctime)  # ุขุฎุฑู ูุงู
                    break
            
            if found_sentiment_file:
                logging.info(f"๐ ูุงู ุงุญุณุงุณุงุช ุงูุช ุดุฏู: {os.path.basename(found_sentiment_file)}")
                try:
                    sentiment_df = pd.read_parquet(found_sentiment_file)
                    logging.info(f"๐ ูุงู ุงุญุณุงุณุงุช ุฎูุงูุฏู ุดุฏ: {sentiment_df.shape}")
                    
                    # ุชูุงุด ุจุฑุง ุงุฏุบุงู ุจุฑ ุงุณุงุณ ุณุงุฎุชุงุฑ ูุงู
                    if 'sentiment_compound_mean' in sentiment_df.columns:
                        logging.info("โ sentiment_compound_mean ุฏุฑ ูุงู ุฎุงุฑุฌ ุงูุช ุดุฏ")
                        # ูพุฑุฏุงุฒุด ุงุฏุบุงู ุจุฑ ุงุณุงุณ index
                        # ฺฉุฏ ุงุฏุบุงู ููุดููุฏ...
                        
                except Exception as e:
                    logging.error(f"โ ุฎุทุง ุฏุฑ ุฎูุงูุฏู ูุงู ุงุญุณุงุณุงุช: {e}")
                    df_features['sentiment_score'] = 0
            else:
                logging.warning("โ๏ธ ูฺ ูุงู ุงุญุณุงุณุงุช ุงูุช ูุดุฏ")
                df_features['sentiment_score'] = 0
        
        # ูุญุงุณุจู ูฺฺฏโูุง ูพุดุฑูุชู ุงุญุณุงุณุงุช
        logging.info("๐งฎ ูุญุงุณุจู ูฺฺฏโูุง ูพุดุฑูุชู ุงุญุณุงุณุงุช...")
        
        def calculate_advanced_sentiment_features(group):
            """ูุญุงุณุจู ูฺฺฏโูุง ูพุดุฑูุชู ุจุฑุง ฺฉ ฺฏุฑูู"""
            # ูุฑุชุจโุณุงุฒ ุจุฑ ุงุณุงุณ ุฒูุงู
            if hasattr(group.index, 'get_level_values') and 'timestamp' in group.index.names:
                group = group.sort_index(level='timestamp')
            elif 'timestamp' in group.columns:
                group = group.sort_values('timestamp')
            else:
                group = group.sort_index()
            
            # ูุญุงุณุจู sentiment_momentum (ุชุบุฑุงุช 24 ุณุงุนุชู)
            momentum_period = min(INDICATOR_PARAMS['sentiment_momentum_period'], len(group))
            if momentum_period > 0:
                group['sentiment_momentum'] = group['sentiment_score'].diff(momentum_period).fillna(0)
            else:
                group['sentiment_momentum'] = 0
            
            # ูุญุงุณุจู ูุงูฺฏู ูุชุญุฑฺฉ ุงุญุณุงุณุงุช
            window_short = min(INDICATOR_PARAMS['sentiment_ma_short'] * 24, len(group))  # 7 ุฑูุฒ * 24 ุณุงุนุช
            window_long = min(INDICATOR_PARAMS['sentiment_ma_long'] * 24, len(group))   # 14 ุฑูุฒ * 24 ุณุงุนุช
            
            # ูุงูฺฏู ูุชุญุฑฺฉ ฺฉูุชุงู ูุฏุช
            if window_short > 0:
                group['sentiment_ma_7'] = group['sentiment_score'].rolling(
                    window=window_short, min_periods=1
                ).mean()
            else:
                group['sentiment_ma_7'] = group['sentiment_score']
            
            # ูุงูฺฏู ูุชุญุฑฺฉ ุจููุฏ ูุฏุช
            if window_long > 0:
                group['sentiment_ma_14'] = group['sentiment_score'].rolling(
                    window=window_long, min_periods=1
                ).mean()
            else:
                group['sentiment_ma_14'] = group['sentiment_score']
            
            # ูุญุงุณุจู sentiment_volume (ุชุนุงูู ุจุง ุญุฌู ูุนุงููุงุช)
            if 'volume' in group.columns:
                # ุชุฑฺฉุจ sentiment ุจุง volume ุจุฑุง ุงุฌุงุฏ sentiment_volume
                sentiment_abs = abs(group['sentiment_score'])
                volume_normalized = group['volume'] / group['volume'].max() if group['volume'].max() > 0 else 0
                group['sentiment_volume'] = sentiment_abs * volume_normalized
                group['sentiment_volume'] = group['sentiment_volume'].rolling(window=24, min_periods=1).sum()
            else:
                group['sentiment_volume'] = abs(group['sentiment_score']).rolling(window=24, min_periods=1).sum()
            
            # ูุญุงุณุจู ูุงฺฏุฑุง ุงุญุณุงุณุงุช ุงุฒ ููุช (ุจูุจูุฏ ุงูุชู)
            if 'close' in group.columns and len(group) > 20:
                try:
                    # ูุฑูุงูโุณุงุฒ ููุช ู ุงุญุณุงุณุงุช
                    price_returns = group['close'].pct_change(20).fillna(0)  # 20-period price change
                    sentiment_change = group['sentiment_score'].diff(20).fillna(0)  # 20-period sentiment change
                    
                    # ูุญุงุณุจู correlation rolling
                    correlation_window = min(50, len(group))
                    if correlation_window > 10:
                        rolling_corr = price_returns.rolling(window=correlation_window, min_periods=10).corr(sentiment_change)
                        group['sentiment_divergence'] = 1 - rolling_corr.fillna(0)  # ูุงฺฏุฑุง = 1 - ููุจุณุชฺฏ
                    else:
                        group['sentiment_divergence'] = 0
                except:
                    group['sentiment_divergence'] = 0
            else:
                group['sentiment_divergence'] = 0
            
            # === ูุญุงุณุจู ูฺฺฏโูุง Reddit ูพุดุฑูุชู ===
            reddit_features = ['reddit_score', 'reddit_comments']
            for feature in reddit_features:
                if feature in group.columns and group[feature].sum() != 0:
                    # ูุญุงุณุจู momentum ุจุฑุง Reddit features
                    group[f'{feature}_momentum'] = group[feature].diff(12).fillna(0)
                    
                    # ูุญุงุณุจู ูุงูฺฏู ูุชุญุฑฺฉ
                    window = INDICATOR_PARAMS.get(f'{feature}_ma', 12)
                    group[f'{feature}_ma'] = group[feature].rolling(window=window, min_periods=1).mean()
                    
                    # ูุญุงุณุจู sentiment-reddit correlation
                    if len(group) > 20:
                        corr_window = min(30, len(group))
                        rolling_corr = group['sentiment_score'].rolling(window=corr_window, min_periods=10).corr(group[feature])
                        group[f'sentiment_{feature}_corr'] = rolling_corr.fillna(0)
                else:
                    group[f'{feature}_momentum'] = 0
                    group[f'{feature}_ma'] = 0
                    group[f'sentiment_{feature}_corr'] = 0
            
            # === ูุญุงุณุจู diversity features ===
            if 'source_diversity' in group.columns:
                max_diversity = group['source_diversity'].max()
                group['source_diversity_normalized'] = group['source_diversity'] / max_diversity if max_diversity > 0 else 0
                
                # ุชุนุงูู diversity ุจุง sentiment
                group['sentiment_diversity_interaction'] = group['sentiment_score'] * group['source_diversity_normalized']
            else:
                group['source_diversity_normalized'] = 0
                group['sentiment_diversity_interaction'] = 0
            
            # ูพุฑ ฺฉุฑุฏู ููุงุฏุฑ NaN
            sentiment_feature_columns = [col for col in group.columns if 'sentiment' in col or 'reddit' in col or 'source' in col]
            for col in sentiment_feature_columns:
                if col in group.columns:
                    group[col] = group[col].fillna(0)
            
            return group
        
        # ุงุนูุงู ูุญุงุณุจุงุช ุจู ูุฑ ฺฏุฑูู
        logging.info("๐ ุงุนูุงู ูุญุงุณุจุงุช ูพุดุฑูุชู ุงุญุณุงุณุงุช...")
        
        if isinstance(df_features.index, pd.MultiIndex):
            if 'symbol' in df_features.index.names and 'timeframe' in df_features.index.names:
                unique_groups = df_features.groupby(level=['symbol', 'timeframe']).ngroups
                logging.info(f"๐ ูพุฑุฏุงุฒุด {unique_groups} ฺฏุฑูู ุจุฑุง ูุญุงุณุจู ุงุญุณุงุณุงุช ูพุดุฑูุชู...")
                
                # ุงุณุชูุงุฏู ุงุฒ group_keys=False ุจุฑุง ุญู pandas deprecation warning
                df_features = df_features.groupby(level=['symbol', 'timeframe'], group_keys=False).apply(
                    calculate_advanced_sentiment_features
                )
            else:
                # ุงฺฏุฑ structure ููุงุณุจ ูุณุชุ ฺฉู ุฏุงุฏู ุฑุง ูพุฑุฏุงุฒุด ฺฉู
                df_features = calculate_advanced_sentiment_features(df_features)
        else:
            # ุงฺฏุฑ MultiIndex ูุณุช
            df_features = calculate_advanced_sentiment_features(df_features)
        
        # ุงุทููุงู ุงุฒ ูุฌูุฏ ููู ูฺฺฏโูุง ุงุญุณุงุณุงุช
        required_sentiment_features = [
            'sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 
            'sentiment_volume', 'sentiment_divergence',
            'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
            'reddit_score_momentum', 'reddit_comments_momentum',
            'sentiment_reddit_score_corr', 'sentiment_reddit_comments_corr',
            'source_diversity_normalized', 'sentiment_diversity_interaction'
        ]
        
        for feature in required_sentiment_features:
            if feature not in df_features.columns:
                df_features[feature] = 0
                logging.warning(f"โ๏ธ {feature} ุงุถุงูู ุดุฏ ุจุง ููุฏุงุฑ ูพุดโูุฑุถ 0")
        
        # ููุงุด ุขูุงุฑ ูฺฺฏโูุง ุงุญุณุงุณุงุช
        logging.info("๐ ุขูุงุฑ ูฺฺฏโูุง ุงุญุณุงุณุงุช:")
        for feature in required_sentiment_features[:6]:  # ููุงุด 6 ูฺฺฏ ุงุตู
            if feature in df_features.columns:
                stats = df_features[feature].describe()
                non_zero = (df_features[feature] != 0).sum()
                logging.info(f"   {feature}: ูุงูฺฏู={stats['mean']:.4f}, ุงูุญุฑุงู ูุนุงุฑ={stats['std']:.4f}, ุบุฑุตูุฑ={non_zero}")
        
        # ููุงุด ุขูุงุฑ Reddit features
        reddit_features_main = ['reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma']
        reddit_stats = {}
        for feature in reddit_features_main:
            if feature in df_features.columns:
                non_zero = (df_features[feature] != 0).sum()
                reddit_stats[feature] = non_zero
        
        if any(reddit_stats.values()):
            logging.info("๐ด ุขูุงุฑ Reddit features:")
            for feature, count in reddit_stats.items():
                logging.info(f"   {feature}: {count} ุฑฺฉูุฑุฏ ุบุฑุตูุฑ")
        else:
            logging.info("๐ด Reddit features: ููู ููุงุฏุฑ ุตูุฑ (Reddit API ุบุฑูุนุงู ุง ุจุฏูู ุฏุงุฏู)")
        
        logging.info("โ ุจูุจูุฏ ูฺฺฏโูุง ุงุญุณุงุณุงุช ุจุง ููููุช ุงูุฌุงู ุดุฏ.")
        
    except Exception as e:
        logging.error(f"โ ุฎุทุง ุฏุฑ ุจูุจูุฏ ูฺฺฏโูุง ุงุญุณุงุณุงุช: {e}")
        import traceback
        logging.error(traceback.format_exc())
        
        # ุงุถุงูู ฺฉุฑุฏู ูฺฺฏโูุง ูพุดโูุฑุถ ุฏุฑ ุตูุฑุช ุฎุทุง
        logging.info("๐ ุงุถุงูู ฺฉุฑุฏู ูฺฺฏโูุง ูพุดโูุฑุถ ุงุญุณุงุณุงุช...")
        required_features = [
            'sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 
            'sentiment_volume', 'sentiment_divergence',
            'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
            'reddit_score_momentum', 'reddit_comments_momentum',
            'sentiment_reddit_score_corr', 'sentiment_reddit_comments_corr',
            'source_diversity_normalized', 'sentiment_diversity_interaction'
        ]
        for feature in required_features:
            if feature not in df_features.columns:
                df_features[feature] = 0
    
    return df_features

def validate_feature_count(df_features: pd.DataFrame) -> Tuple[int, List[str]]:
    """ุจุฑุฑุณ ุชุนุฏุงุฏ ูฺฺฏโูุง ู ุดูุงุณุง ูฺฺฏโูุง ุงุฒ ุฏุณุช ุฑูุชู"""
    exclude_cols = ['timestamp', 'symbol', 'timeframe', 'open', 'high', 'low', 'close', 'volume', 'target']
    
    if isinstance(df_features.index, pd.MultiIndex):
        # ุงุฒ index names ุญุฐู ฺฉู
        index_cols = list(df_features.index.names) if df_features.index.names else []
        exclude_cols.extend(index_cols)
    
    feature_columns = [col for col in df_features.columns if col not in exclude_cols]
    
    # ูุณุช ูฺฺฏโูุง ููุฑุฏ ุงูุชุธุงุฑ (58 ูฺฺฏ)
    expected_features = [
        # Technical indicators (43 features)
        'rsi', 'macd', 'macd_hist', 'macd_signal',
        'bb_upper', 'bb_middle', 'bb_lower', 'bb_position',
        'atr', 'atr_percent', 'volatility', 'price_change',
        'vwap', 'vwap_deviation', 'obv', 'obv_change', 'mfi', 'ad',
        'stoch_k', 'stoch_d', 'williams_r', 'cci',
        'ema_short', 'ema_medium', 'ema_long', 'ema_short_above_medium', 'ema_medium_above_long',
        'ema_short_slope', 'ema_medium_slope',
        'sma_short', 'sma_medium', 'sma_long', 'price_above_sma_short', 'price_above_sma_medium', 'price_above_sma_long',
        'return_1', 'return_5', 'return_10', 'avg_return_5', 'avg_return_10',
        'hl_ratio', 'close_position', 'volume_ma', 'volume_ratio',
        'psar', 'price_above_psar', 'adx',
        
        # Sentiment features (15 features)
        'sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence',
        'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
        'reddit_score_momentum', 'reddit_comments_momentum',
        'sentiment_reddit_score_corr', 'sentiment_reddit_comments_corr',
        'source_diversity_normalized', 'sentiment_diversity_interaction'
    ]
    
    missing_features = [f for f in expected_features if f not in feature_columns]
    
    return len(feature_columns), missing_features

def run_feature_engineering(input_path: str, output_path: str):
    """ุชุงุจุน ุงุตู ุงุฌุฑุง ูููุฏุณ ูฺฺฏ - ูุณุฎู ฺฉุงููุงู ุงุตูุงุญ ุดุฏู"""
    global GLOBAL_COUNTER, TOTAL_GROUPS
    
    start_time = datetime.now()
    logging.info("๐ ุดุฑูุน ูููุฏุณ ูฺฺฏ (ูุณุฎู ฺฉุงููุงู ุงุตูุงุญ ุดุฏู ุจุฑุง ุณุงุฒฺฏุงุฑ ุจุง ูุงูโูุง 01 ู 02)...")
    logging.info(f"๐ ูพุงุฑุงูุชุฑูุง ุงูุฏฺฉุงุชูุฑ: {INDICATOR_PARAMS}")
    
    # ุงูุชู ุขุฎุฑู ูุงู ุฏุงุฏู
    list_of_files = glob.glob(os.path.join(input_path, 'master_*_data_*.parquet'))
    if not list_of_files:
        logging.error(f"โ ูฺ ูุงู ุฏุงุฏู ุงุตู ุฏุฑ ูุณุฑ '{input_path}' ุงูุช ูุดุฏ.")
        return
    latest_file = max(list_of_files, key=os.path.getctime)
    logging.info(f"๐ ุฏุฑ ุญุงู ุฎูุงูุฏู ูุงู ุฏุงุฏู ุงุตู: {os.path.basename(latest_file)}")
    
    # ุฎูุงูุฏู ุฏุงุฏู
    df = pd.read_parquet(latest_file)
    logging.info(f"๐ ุชุนุฏุงุฏ ุฑุฏูโูุง ุงููู: {len(df):,}")
    
    # ุจุฑุฑุณ ุณุงุฎุชุงุฑ ูุงู
    logging.info(f"๐ ุณุงุฎุชุงุฑ ูุงู: Index={df.index.names}, Columns={list(df.columns)}")
    
    # ุจุฑุฑุณ ูุฌูุฏ ุณุชููโูุง Broadcasting sentiment 
    sentiment_cols = [col for col in df.columns if 'sentiment' in col]
    reddit_cols = [col for col in df.columns if 'reddit' in col]
    source_cols = [col for col in df.columns if 'source' in col]
    
    logging.info(f"๐ญ ุณุชููโูุง ุงุญุณุงุณุงุช ููุฌูุฏ: {sentiment_cols}")
    logging.info(f"๐ด ุณุชููโูุง Reddit ููุฌูุฏ: {reddit_cols}")
    logging.info(f"๐ก ุณุชููโูุง Source ููุฌูุฏ: {source_cols}")
    
    # ูุญุงุณุจู ุชุนุฏุงุฏ ฺฉู ฺฏุฑููโูุง
    if isinstance(df.index, pd.MultiIndex) and 'symbol' in df.index.names and 'timeframe' in df.index.names:
        TOTAL_GROUPS = df.groupby(level=['symbol', 'timeframe']).ngroups
    else:
        TOTAL_GROUPS = 1
    logging.info(f"๐ข ุชุนุฏุงุฏ ฺฉู ฺฏุฑููโูุง ุจุฑุง ูพุฑุฏุงุฒุด: {TOTAL_GROUPS:,}")
    
    # ุงุนูุงู ูฺฺฏโูุง
    logging.info("โ๏ธ ุดุฑูุน ุงุนูุงู ูฺฺฏโูุง ุจู ุตูุฑุช ฺฏุฑููโุจูุฏ ุดุฏู...")
    logging.info(f"๐ก ุญุฏุงูู ุฏุงุฏู ููุฑุฏ ูุงุฒ ุจุฑุง ูุฑ ฺฏุฑูู: {INDICATOR_PARAMS['min_data_points']}")
    
    if isinstance(df.index, pd.MultiIndex) and 'symbol' in df.index.names and 'timeframe' in df.index.names:
        # ุงุณุชูุงุฏู ุงุฒ group_keys=False ุจุฑุง ุญู pandas deprecation warning
        df_features = df.groupby(level=['symbol', 'timeframe'], group_keys=False).apply(apply_features)
    else:
        # ุงฺฏุฑ ุณุงุฎุชุงุฑ MultiIndex ุฏุฑุณุช ูุณุชุ ฺฉู ุฏุงุฏู ุฑุง ูพุฑุฏุงุฒุด ฺฉู
        logging.warning("โ๏ธ ุณุงุฎุชุงุฑ MultiIndex ููุงุณุจ ูุณุชุ ูพุฑุฏุงุฒุด ฺฉู ุฏุงุฏู...")
        df_features = apply_features(df)
        if df_features is None:
            logging.error("โ ุฎุทุง ุฏุฑ ูุญุงุณุจู ูฺฺฏโูุง")
            return
    
    # ูพุงฺฉุณุงุฒ ุญุงูุธู
    del df
    gc.collect()
    
    # ุจุฑุฑุณ ูุชุฌู
    if df_features is None or (isinstance(df_features, pd.DataFrame) and df_features.empty):
        logging.error("โ ุฎุทุง ุฏุฑ ูุญุงุณุจู ูฺฺฏโูุง. ุนููุงุช ูุชููู ุดุฏ.")
        return
    
    # ุญุฐู ฺฏุฑููโูุง ฺฉู ุจู ุฏูู ูุฏุงุดุชู ุฏุงุฏู ฺฉุงูุ None ุจุฑฺฏุฑุฏุงูุฏูโุงูุฏ
    if isinstance(df_features, pd.DataFrame):
        initial_rows = len(df_features)
        df_features.dropna(how='all', inplace=True)
        final_rows_after_filter = len(df_features)
        logging.info(f"๐ ุชุนุฏุงุฏ ุฑุฏูโูุง ูพุณ ุงุฒ ูุญุงุณุจู ูฺฺฏโูุง: {final_rows_after_filter:,} (ุญุฐู ุดุฏู: {initial_rows - final_rows_after_filter:,})")
    
    # --- ุจุฎุด ุจูุจูุฏ ุงูุชู: ุงุฏุบุงู ุฏุงุฏูโูุง ุงุญุณุงุณุงุช ---
    df_features = enhance_sentiment_features(df_features, input_path)

    logging.info("๐ฏ ูุญุงุณุจู ูฺฺฏโูุง ุชูุงู ุดุฏ. ุฏุฑ ุญุงู ุงุฌุงุฏ ูุชุบุฑ ูุฏู...")
    
    # ุงุทููุงู ุงุฒ ูุฌูุฏ index ุตุญุญ
    if not isinstance(df_features.index, pd.MultiIndex):
        if 'symbol' in df_features.columns and 'timeframe' in df_features.columns and 'timestamp' in df_features.columns:
            df_features.set_index(['symbol', 'timeframe', 'timestamp'], inplace=True)
        else:
            logging.warning("โ๏ธ ููโุชูุงู index ุตุญุญ ุชูุธู ฺฉุฑุฏ")
    
    # ูุญุงุณุจู ูุชุบุฑ ูุฏู ุจุง ุฑูุด ูพูุง
    logging.info(f"๐ฎ ูุญุงุณุจู target ุจุง {TARGET_FUTURE_PERIODS} ูพุฑูุฏ ุขูุฏู ู {TARGET_PROFIT_PERCENT*100}% ุณูุฏ")
    
    if isinstance(df_features.index, pd.MultiIndex) and 'symbol' in df_features.index.names and 'timeframe' in df_features.index.names:
        df_features['future_close'] = df_features.groupby(level=['symbol', 'timeframe'])['close'].shift(-TARGET_FUTURE_PERIODS)
    else:
        df_features['future_close'] = df_features['close'].shift(-TARGET_FUTURE_PERIODS)
    
    df_features['target'] = (df_features['future_close'] > df_features['close'] * (1 + TARGET_PROFIT_PERCENT)).astype(int)
    
    # ุญุฐู ุณุชูู ฺฉูฺฉ ู ุฑุฏูโูุง ูุงูุต
    df_features.drop(columns=['future_close'], inplace=True)
    
    # === ุจุฑุฑุณ ุชุนุฏุงุฏ ูฺฺฏโูุง ===
    feature_count, missing_features = validate_feature_count(df_features)
    logging.info(f"๐ข ุชุนุฏุงุฏ ูฺฺฏโูุง ูุญุงุณุจู ุดุฏู: {feature_count}")
    
    if missing_features:
        logging.warning(f"โ๏ธ ูฺฺฏโูุง ุงุฒ ุฏุณุช ุฑูุชู ({len(missing_features)}): {missing_features}")
    else:
        logging.info("โ ููู ูฺฺฏโูุง ููุฑุฏ ุงูุชุธุงุฑ ูุญุงุณุจู ุดุฏูโุงูุฏ")
    
    # ุญุฐู ุฑุฏูโูุง ุฏุงุฑุง ููุฏุงุฑ NaN
    initial_rows = len(df_features)
    df_features.dropna(inplace=True)
    final_rows = len(df_features)
    logging.info(f"๐งน ุชุนุฏุงุฏ ุฑุฏูโูุง ุญุฐู ุดุฏู ุจู ุฏูู NaN: {initial_rows - final_rows:,}")
    
    logging.info(f"โ ุฏุชุงุณุช ููุง ุจุง {final_rows:,} ุฑุฏู ู {feature_count} ูฺฺฏ ุขูุงุฏู ุดุฏ.")
    
    # ููุงุด ุขูุงุฑ ฺฉู ูฺฺฏโูุง (ุจุง ุชุงฺฉุฏ ุจุฑ ูฺฺฏโูุง ุงุญุณุงุณุงุช)
    logging.info("๐ === ุขูุงุฑ ฺฉู ูฺฺฏโูุง ===")
    important_features = ['sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 
                         'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence',
                         'reddit_score', 'reddit_comments',
                         'rsi', 'macd', 'bb_position', 'atr_percent', 'volume_ratio']
    
    for col in important_features:
        if col in df_features.columns:
            mean_val = df_features[col].mean()
            std_val = df_features[col].std()
            non_zero = (df_features[col] != 0).sum()
            logging.info(f"   {col}: ูุงูฺฏู={mean_val:.4f}, ุงูุญุฑุงู ูุนุงุฑ={std_val:.4f}, ุบุฑุตูุฑ={non_zero}")
    
    # ุจุฑุฑุณ ุชูุฒุน target
    target_distribution = df_features['target'].value_counts()
    logging.info(f"๐ฏ ุชูุฒุน ูุชุบุฑ ูุฏู: {target_distribution.to_dict()}")
    target_percentage = (target_distribution.get(1, 0) / len(df_features)) * 100
    logging.info(f"๐ ุฏุฑุตุฏ ูููููโูุง ูุซุจุช: {target_percentage:.2f}%")
    
    # ุฐุฎุฑู ุฏุชุงุณุช ููุง
    logging.info("๐พ ุดุฑูุน ุฐุฎุฑูโุณุงุฒ...")
    os.makedirs(output_path, exist_ok=True)
    timestamp_str = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
    
    # ุฐุฎุฑู ูุงู ุงุตู
    output_filename = f'final_dataset_for_training_{timestamp_str}.parquet'
    output_file_path = os.path.join(output_path, output_filename)
    df_features.to_parquet(output_file_path)
    logging.info(f"โ ุฏุชุงุณุช ููุง ุขูุงุฏู ุขููุฒุด ุฏุฑ ูุณุฑ '{output_file_path}' (ูุฑูุช Parquet) ุฐุฎุฑู ุดุฏ.")
    
    # ุฐุฎุฑู ูุงู CSV ุจุฑุง ุจุฑุฑุณ ุฏุณุช
    csv_output_filename = f'final_dataset_for_training_{timestamp_str}.csv'
    csv_output_file_path = os.path.join(output_path, csv_output_filename)
    sample_size = min(1000, len(df_features))
    df_features.head(sample_size).to_csv(csv_output_file_path, index=True)
    logging.info(f"๐ ููููู CSV ({sample_size:,} ุฑุฏู ุงูู) ุฏุฑ ูุณุฑ '{csv_output_file_path}' ุฐุฎุฑู ุดุฏ.")
    
    # ูุญุงุณุจู ุฒูุงู ุงุฌุฑุง
    end_time = datetime.now()
    execution_time = end_time - start_time
    logging.info(f"โฑ๏ธ ุฒูุงู ุงุฌุฑุง ฺฉู: {execution_time}")
    
    # ฺฏุฒุงุฑุด ููุง
    print("\n" + "="*80)
    print("๐ === ฺฏุฒุงุฑุด ููุง ูููุฏุณ ูฺฺฏ (ูุณุฎู ฺฉุงููุงู ุงุตูุงุญ ุดุฏู) ===")
    print(f"๐ ุชุนุฏุงุฏ ฺฉู ุฑุฏูโูุง: {final_rows:,}")
    print(f"๐ข ุชุนุฏุงุฏ ูฺฺฏโูุง: {feature_count}")
    print(f"๐ฏ ุฏุฑุตุฏ ูููููโูุง ูุซุจุช: {target_percentage:.2f}%")
    print(f"โฑ๏ธ ุฒูุงู ุงุฌุฑุง: {execution_time}")
    print(f"๐ ูุงู ุฎุฑูุฌ: {output_filename}")
    print("\n๐ ูฺฺฏโูุง ุงุญุณุงุณุงุช (Broadcasting + Multi-source):")
    print("  โ sentiment_score (ุงูุชุงุฒ ูพุงู ุงุฒ Broadcasting)")
    print("  โ sentiment_momentum (ุชุบุฑุงุช ูุญุงุณุจู ุดุฏู)")
    print("  โ sentiment_ma_7, sentiment_ma_14 (ูุงูฺฏู ูุชุญุฑฺฉ)")
    print("  โ sentiment_volume (ุญุฌู ุชุนุงูู)")
    print("  โ sentiment_divergence (ูุงฺฏุฑุง ุงุฒ ููุช)")
    print("\n๐ด ูฺฺฏโูุง Reddit (ุฌุฏุฏ):")
    print("  โ reddit_score, reddit_comments (ุงูุชุงุฒุงุช Reddit)")
    print("  โ reddit_score_ma, reddit_comments_ma (ูุงูฺฏู ูุชุญุฑฺฉ)")
    print("  โ reddit_score_momentum, reddit_comments_momentum (ุชุญุฑฺฉ)")
    print("  โ sentiment_reddit_*_corr (ููุจุณุชฺฏ ุจุง ุงุญุณุงุณุงุช)")
    print("\n๐ก ูฺฺฏโูุง Source Diversity:")
    print("  โ source_diversity_normalized (ุชููุน ููุงุจุน ูุฑูุงู ุดุฏู)")
    print("  โ sentiment_diversity_interaction (ุชุนุงูู ุงุญุณุงุณุงุช ู ุชููุน)")
    print("\n๐ง ุงุตูุงุญุงุช ูู:")
    print("  โ ุฑูุน ูุดฺฉู PSAR missing")
    print("  โ ุญู pandas deprecation warnings")
    print("  โ ุจูุจูุฏ MFI calculation")
    print("  โ ุจูููโุณุงุฒ memory management")
    print("="*80)
    
    # ููุงุด ููููู ุฏุงุฏู ููุง
    if final_rows > 0:
        print("\n--- ููููู ต ุฑุฏู ุขุฎุฑ ุงุฒ ุฏุชุงุณุช ููุง ---")
        display_cols = ['open', 'high', 'low', 'close', 'volume', 'target'] + \
                      [col for col in ['sentiment_score', 'reddit_score', 'rsi', 'macd', 'bb_position'] if col in df_features.columns][:5]
        print(df_features[display_cols].tail())
        
        print(f"\n--- ุงุทูุงุนุงุช ฺฉู ุฏุชุงุณุช ููุง ---")
        print(f"Shape: {df_features.shape}")
        print(f"Memory usage: {df_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # ููุงุด ุขูุงุฑ sentiment features
        sentiment_stats = {}
        for col in ['sentiment_score', 'reddit_score', 'reddit_comments']:
            if col in df_features.columns:
                non_zero = (df_features[col] != 0).sum()
                sentiment_stats[col] = non_zero
        
        if sentiment_stats:
            print(f"\n--- ุขูุงุฑ ูฺฺฏโูุง ุงุญุณุงุณุงุช ---")
            for col, count in sentiment_stats.items():
                percentage = (count / len(df_features)) * 100
                print(f"{col}: {count:,} ุบุฑุตูุฑ ({percentage:.1f}%)")

if __name__ == '__main__':
    run_feature_engineering(PROCESSED_DATA_PATH, FEATURES_PATH)