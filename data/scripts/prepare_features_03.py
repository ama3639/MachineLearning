#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ (ÙØ§Ø² Û³ØŒ Ú¯Ø§Ù… Ø§Ù„Ù) - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ú©Ø§Ù…Ù„

ğŸ”§ ØªØºÛŒÛŒØ±Ø§Øª Ù…Ù‡Ù… Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡:
- âœ… Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ 01 Ùˆ 02 Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
- âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø² Broadcasting sentiment structure
- âœ… Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Telegram features support (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Reddit)
- âœ… Ø±ÙØ¹ Ù…Ø´Ú©Ù„ PSAR calculation Ùˆ count Ù…Ø´Ú©Ù„ 57/58
- âœ… Ø­Ù„ pandas deprecation warnings
- âœ… Ø¨Ù‡Ø¨ÙˆØ¯ error handling Ùˆ fallback mechanisms  
- âœ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ memory management
- âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² multi-source sentiment (GNews, NewsAPI, CoinGecko, RSS, Telegram)
- âœ… Ø¨Ù‡Ø¨ÙˆØ¯ comprehensive logging
- âœ… Ø§ØµÙ„Ø§Ø­ MFI calculation warnings
- âœ… Ø¨Ù‡Ø¨ÙˆØ¯ feature alignment Ùˆ time-series processing
- ğŸ†• ØªØ´Ø®ÛŒØµ ØµØ­ÛŒØ­ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ sentiment_compound_mean Ø§Ø² ÙØ§ÛŒÙ„ 02
- ğŸ†• Ù†Ú¯Ø§Ø´Øª ØµØ­ÛŒØ­ Broadcasting sentiment Ø¨Ù‡ Point-in-Time
- ğŸ†• Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Reddit features Ø¨Ø§ Telegram features
- ğŸ†• Ø¨Ù‡Ø¨ÙˆØ¯ enhance_sentiment_features Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„

ØªØºÛŒÛŒØ±Ø§Øª Ø§ØµÙ„ÛŒ:
- Ø­Ù„ Ù…Ø´Ú©Ù„ sentiment_score = 0 Ø¨Ø§ Ø®ÙˆØ§Ù†Ø¯Ù† ØµØ­ÛŒØ­ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Broadcasting
- Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Telegram-specific features Ø¨Ù‡ Ø¬Ø§ÛŒ Reddit
- Ø±ÙØ¹ Ù…Ø´Ú©Ù„ PSAR missing
- Ø¨Ù‡Ø¨ÙˆØ¯ multi-source sentiment processing
- Ø§ØµÙ„Ø§Ø­ pandas compatibility issues
- ğŸ†• ØªØ·Ø¨ÛŒÙ‚ Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø®Ø±ÙˆØ¬ÛŒ ÙØ§ÛŒÙ„ 02 Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
"""
import os
import glob
import pandas as pd
import pandas_ta as ta
import logging
import configparser
from typing import Optional, Dict, Any, List, Tuple
import numpy as np
import gc
from datetime import datetime
import warnings

# ØªÙ†Ø¸ÛŒÙ… warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)

# Ø¨Ø®Ø´ Ø®ÙˆØ§Ù†Ø¯Ù† Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
config = configparser.ConfigParser()
CONFIG_FILE_PATH = 'config.ini'
try:
    config.read(CONFIG_FILE_PATH, encoding='utf-8')
    PROCESSED_DATA_PATH = config.get('Paths', 'processed')
    FEATURES_PATH = config.get('Paths', 'features')
    LOG_PATH = config.get('Paths', 'logs')
except Exception as e:
    print(f"CRITICAL ERROR: Could not read 'config.ini'. Error: {e}")
    exit()

# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)
os.makedirs(FEATURES_PATH, exist_ok=True)
os.makedirs(LOG_PATH, exist_ok=True)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ù¾ÙˆÛŒØ§ ---
script_name = os.path.splitext(os.path.basename(__file__))[0]
log_subfolder_path = os.path.join(LOG_PATH, script_name)
os.makedirs(log_subfolder_path, exist_ok=True)

log_filename = os.path.join(log_subfolder_path, f"log_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])

# === Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾ÙˆÛŒØ§ Ùˆ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ… (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡) ===
INDICATOR_PARAMS = {
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    'rsi_length': 14,
    'macd_fast': 12,
    'macd_slow': 26,
    'macd_signal': 9,
    'bb_length': 20,
    'bb_std': 2.0,
    
    # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
    'atr_length': 14,
    'vwap_anchor': None,  # None Ø¨Ø±Ø§ÛŒ VWAP Ø±ÙˆØ²Ø§Ù†Ù‡
    
    # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù† Ùˆ ØªØ±Ù†Ø¯
    'stoch_k': 14,
    'stoch_d': 3,
    'stoch_smooth': 3,
    'williams_r_length': 14,
    'cci_length': 20,
    
    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªØ­Ø±Ú©
    'ema_short': 12,
    'ema_medium': 26,
    'ema_long': 50,
    'sma_short': 10,
    'sma_medium': 20,
    'sma_long': 50,
    
    # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø­Ø¬Ù…
    'obv_enabled': True,
    'mfi_length': 14,
    'ad_enabled': True,
    
    # === Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ ===
    'sentiment_ma_short': 7,
    'sentiment_ma_long': 14,
    'sentiment_momentum_period': 24,  # 24 Ø³Ø§Ø¹Øª
    
    # ğŸ†• === Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Telegram (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Reddit) ===
    'telegram_sentiment_ma': 12,  # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© telegram sentiment
    'telegram_momentum_period': 24,  # Ø¯ÙˆØ±Ù‡ momentum Ø¨Ø±Ø§ÛŒ telegram
    
    # Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
    'min_data_points': 100,
    
    # === Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ PSAR (Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡) ===
    'psar_af': 0.02,
    'psar_max_af': 0.2,
}

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ùˆ Ù‡Ø¯Ù
TARGET_FUTURE_PERIODS = 24
TARGET_PROFIT_PERCENT = 0.02

# Ú©Ø§Ù†ØªØ± global Ø¨Ø±Ø§ÛŒ tracking
GLOBAL_COUNTER = 0
TOTAL_GROUPS = 0

def log_indicator_error(indicator_name: str, group_name: Any, error: Exception):
    """ØªØ§Ø¨Ø¹ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§"""
    logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ {indicator_name} Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙˆÙ‡ {group_name}: {error}")

def log_progress(current: int, total: int, group_name: str = ""):
    """Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª Ù¾Ø±Ø¯Ø§Ø²Ø´"""
    if total > 0:
        progress = (current / total) * 100
        if current % max(1, total // 20) == 0:  # Ù‡Ø± 5% Ú¯Ø²Ø§Ø±Ø´
            logging.info(f"ğŸ”„ Ù¾ÛŒØ´Ø±ÙØª: {progress:.1f}% ({current}/{total}) - {group_name}")

def safe_numeric_conversion(series: pd.Series, name: str) -> pd.Series:
    """ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ…Ù† Ø¨Ù‡ numeric Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
    try:
        return pd.to_numeric(series, errors='coerce')
    except Exception as e:
        logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ {name} Ø¨Ù‡ numeric: {e}")
        return series.fillna(0)

def apply_features(group: pd.DataFrame) -> Optional[pd.DataFrame]:
    """
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ØªÙ…Ø§Ù… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú¯Ø±ÙˆÙ‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ 01 Ùˆ 02
    """
    global GLOBAL_COUNTER, TOTAL_GROUPS
    GLOBAL_COUNTER += 1
    
    # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
    log_progress(GLOBAL_COUNTER, TOTAL_GROUPS, str(group.name))
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
    if len(group) < INDICATOR_PARAMS['min_data_points']:
        logging.debug(f"Ú¯Ø±ÙˆÙ‡ {group.name} Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±Ø¯ ({len(group)} < {INDICATOR_PARAMS['min_data_points']})")
        return None

    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ ØµØ­ÛŒØ­ Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡
    numeric_columns = ['open', 'high', 'low', 'close', 'volume']
    for col in numeric_columns:
        if col in group.columns:
            group[col] = safe_numeric_conversion(group[col], col)

    # === Ø¨Ø®Ø´ Û±: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªØ±Ù†Ø¯ Ùˆ Ù‚ÛŒÙ…Øª ===
    try:
        group['rsi'] = ta.rsi(group['close'], length=INDICATOR_PARAMS['rsi_length'])
    except Exception as e:
        log_indicator_error('RSI', group.name, e)

    try:
        macd = ta.macd(group['close'], 
                      fast=INDICATOR_PARAMS['macd_fast'], 
                      slow=INDICATOR_PARAMS['macd_slow'], 
                      signal=INDICATOR_PARAMS['macd_signal'])
        if macd is not None and not macd.empty:
            group['macd'] = macd[f'MACD_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
            group['macd_hist'] = macd[f'MACDh_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
            group['macd_signal'] = macd[f'MACDs_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
    except Exception as e:
        log_indicator_error('MACD', group.name, e)

    try:
        bbands = ta.bbands(group['close'], 
                          length=INDICATOR_PARAMS['bb_length'], 
                          std=INDICATOR_PARAMS['bb_std'])
        if bbands is not None and not bbands.empty:
            group['bb_upper'] = bbands[f'BBU_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            group['bb_middle'] = bbands[f'BBM_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            group['bb_lower'] = bbands[f'BBL_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚ÛŒÙ…Øª Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ Bollinger Bands
            bb_range = group['bb_upper'] - group['bb_lower']
            group['bb_position'] = np.where(bb_range != 0, 
                                          (group['close'] - group['bb_lower']) / bb_range, 
                                          0.5)
    except Exception as e:
        log_indicator_error('Bollinger Bands', group.name, e)

    # === Ø¨Ø®Ø´ Û²: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù† (Volatility) ===
    try:
        group['atr'] = ta.atr(group['high'], group['low'], group['close'], 
                             length=INDICATOR_PARAMS['atr_length'])
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡ (ATR Ø¨Ù‡ Ù†Ø³Ø¨Øª Ù‚ÛŒÙ…Øª)
        group['atr_percent'] = np.where(group['close'] != 0, 
                                      (group['atr'] / group['close']) * 100, 
                                      0)
    except Exception as e:
        log_indicator_error('ATR', group.name, e)

    try:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†ÙˆØ³Ø§Ù† ØªØ§Ø±ÛŒØ®ÛŒ (Historical Volatility)
        group['price_change'] = group['close'].pct_change()
        group['volatility'] = group['price_change'].rolling(window=20).std() * 100
    except Exception as e:
        log_indicator_error('Historical Volatility', group.name, e)

    # === Ø¨Ø®Ø´ Û³: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø­Ø¬Ù… (Volume-Based) - Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ ===
    try:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ VWAP Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ MultiIndex
        typical_price = (group['high'] + group['low'] + group['close']) / 3
        vwap_numerator = (typical_price * group['volume']).cumsum()
        vwap_denominator = group['volume'].cumsum()
        # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±
        group['vwap'] = np.where(vwap_denominator != 0, 
                               vwap_numerator / vwap_denominator, 
                               typical_price)
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø­Ø±Ø§Ù Ù‚ÛŒÙ…Øª Ø§Ø² VWAP
        group['vwap_deviation'] = np.where(group['vwap'] != 0,
                                         ((group['close'] - group['vwap']) / group['vwap']) * 100,
                                         0)
    except Exception as e:
        log_indicator_error('VWAP', group.name, e)

    if INDICATOR_PARAMS['obv_enabled']:
        try:
            group['obv'] = ta.obv(group['close'], group['volume'])
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª OBV
            group['obv_change'] = group['obv'].pct_change().fillna(0)
        except Exception as e:
            log_indicator_error('OBV', group.name, e)

    try:
        # === Ø§ØµÙ„Ø§Ø­ MFI calculation Ø¨Ø±Ø§ÛŒ Ø­Ù„ warning ===
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ ØµØ­ÛŒØ­
        high_safe = safe_numeric_conversion(group['high'], 'high')
        low_safe = safe_numeric_conversion(group['low'], 'low')
        close_safe = safe_numeric_conversion(group['close'], 'close')
        volume_safe = safe_numeric_conversion(group['volume'], 'volume')
        
        group['mfi'] = ta.mfi(high_safe, low_safe, close_safe, volume_safe, 
                             length=INDICATOR_PARAMS['mfi_length'])
    except Exception as e:
        log_indicator_error('MFI', group.name, e)

    if INDICATOR_PARAMS['ad_enabled']:
        try:
            group['ad'] = ta.ad(group['high'], group['low'], group['close'], group['volume'])
        except Exception as e:
            log_indicator_error('A/D Line', group.name, e)

    # === Ø¨Ø®Ø´ Û´: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§ÙˆØ³ÛŒÙ„Ø§ØªÙˆØ± ===
    try:
        stoch = ta.stoch(group['high'], group['low'], group['close'], 
                        k=INDICATOR_PARAMS['stoch_k'], 
                        d=INDICATOR_PARAMS['stoch_d'], 
                        smooth_k=INDICATOR_PARAMS['stoch_smooth'])
        if stoch is not None and not stoch.empty:
            group['stoch_k'] = stoch[f'STOCHk_{INDICATOR_PARAMS["stoch_k"]}_{INDICATOR_PARAMS["stoch_d"]}_{INDICATOR_PARAMS["stoch_smooth"]}']
            group['stoch_d'] = stoch[f'STOCHd_{INDICATOR_PARAMS["stoch_k"]}_{INDICATOR_PARAMS["stoch_d"]}_{INDICATOR_PARAMS["stoch_smooth"]}']
    except Exception as e:
        log_indicator_error('Stochastic', group.name, e)

    try:
        group['williams_r'] = ta.willr(group['high'], group['low'], group['close'], 
                                      length=INDICATOR_PARAMS['williams_r_length'])
    except Exception as e:
        log_indicator_error('Williams %R', group.name, e)

    try:
        group['cci'] = ta.cci(group['high'], group['low'], group['close'], 
                             length=INDICATOR_PARAMS['cci_length'])
    except Exception as e:
        log_indicator_error('CCI', group.name, e)

    # === Ø¨Ø®Ø´ Ûµ: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªØ­Ø±Ú© ===
    try:
        group['ema_short'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_short'])
        group['ema_medium'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_medium'])
        group['ema_long'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_long'])
        
        # Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ cross over
        group['ema_short_above_medium'] = (group['ema_short'] > group['ema_medium']).astype(int)
        group['ema_medium_above_long'] = (group['ema_medium'] > group['ema_long']).astype(int)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÛŒØ¨ EMA (ØªØ±Ù†Ø¯)
        group['ema_short_slope'] = group['ema_short'].pct_change(periods=5).fillna(0)
        group['ema_medium_slope'] = group['ema_medium'].pct_change(periods=5).fillna(0)
    except Exception as e:
        log_indicator_error('EMA', group.name, e)

    try:
        group['sma_short'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_short'])
        group['sma_medium'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_medium'])
        group['sma_long'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_long'])
        
        # Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚ÛŒÙ…Øª Ù†Ø³Ø¨Øª Ø¨Ù‡ SMA
        group['price_above_sma_short'] = (group['close'] > group['sma_short']).astype(int)
        group['price_above_sma_medium'] = (group['close'] > group['sma_medium']).astype(int)
        group['price_above_sma_long'] = (group['close'] > group['sma_long']).astype(int)
    except Exception as e:
        log_indicator_error('SMA', group.name, e)

    # === Ø¨Ø®Ø´ Û¶: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ø®Ø§Ù… ===
    try:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        group['return_1'] = group['close'].pct_change(1).fillna(0)
        group['return_5'] = group['close'].pct_change(5).fillna(0)
        group['return_10'] = group['close'].pct_change(10).fillna(0)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø§Ø²Ø¯Ù‡
        group['avg_return_5'] = group['return_1'].rolling(window=5, min_periods=1).mean()
        group['avg_return_10'] = group['return_1'].rolling(window=10, min_periods=1).mean()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ High-Low ratio
        group['hl_ratio'] = np.where(group['close'] != 0,
                                   (group['high'] - group['low']) / group['close'],
                                   0)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª close Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ high-low
        hl_range = group['high'] - group['low']
        group['close_position'] = np.where(hl_range != 0,
                                         (group['close'] - group['low']) / hl_range,
                                         0.5)
        
        # Ø­Ø¬Ù… Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡
        group['volume_ma'] = group['volume'].rolling(window=20, min_periods=1).mean()
        group['volume_ratio'] = np.where(group['volume_ma'] != 0,
                                       group['volume'] / group['volume_ma'],
                                       1.0)
        
    except Exception as e:
        log_indicator_error('Price Features', group.name, e)

    # === Ø¨Ø®Ø´ Û·: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ùˆ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù‡) ===
    try:
        # === Parabolic SAR Ø¨Ø§ Ø§ØµÙ„Ø§Ø­ Ú©Ø§Ù…Ù„ ===
        psar_result = ta.psar(group['high'], group['low'], group['close'], 
                             af0=INDICATOR_PARAMS['psar_af'], 
                             af=INDICATOR_PARAMS['psar_af'], 
                             max_af=INDICATOR_PARAMS['psar_max_af'])
        if psar_result is not None:
            if isinstance(psar_result, pd.DataFrame):
                # Ø§Ú¯Ø± DataFrame Ø§Ø³ØªØŒ Ø³ØªÙˆÙ† Ø§ÙˆÙ„ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                if len(psar_result.columns) > 0:
                    group['psar'] = psar_result.iloc[:, 0]
                else:
                    group['psar'] = group['close']  # fallback
            else:
                # Ø§Ú¯Ø± Series Ø§Ø³Øª
                group['psar'] = psar_result
            
            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ PSAR
            if 'psar' in group.columns:
                group['price_above_psar'] = (group['close'] > group['psar']).astype(int)
            else:
                group['psar'] = group['close']  # fallback
                group['price_above_psar'] = 0
        else:
            # Ø§Ú¯Ø± PSAR Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø´Ø¯ØŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            group['psar'] = group['close']
            group['price_above_psar'] = 0
            
    except Exception as e:
        log_indicator_error('Parabolic SAR', group.name, e)
        # fallback Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        group['psar'] = group['close']
        group['price_above_psar'] = 0

    try:
        # ADX Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ù†ØªÛŒØ¬Ù‡
        adx_result = ta.adx(group['high'], group['low'], group['close'], length=14)
        if adx_result is not None:
            if isinstance(adx_result, pd.DataFrame):
                # Ø§Ú¯Ø± DataFrame Ø§Ø³ØªØŒ Ø³ØªÙˆÙ† ADX Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                if 'ADX_14' in adx_result.columns:
                    group['adx'] = adx_result['ADX_14']
                else:
                    group['adx'] = adx_result.iloc[:, 0]  # Ø³ØªÙˆÙ† Ø§ÙˆÙ„
            else:
                # Ø§Ú¯Ø± Series Ø§Ø³Øª
                group['adx'] = adx_result
        else:
            group['adx'] = 50  # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ ADX

    except Exception as e:
        log_indicator_error('ADX', group.name, e)
        group['adx'] = 50  # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶

    # === Ø¨Ø®Ø´ Û¸: Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ (Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡) ===
    try:
        # ğŸ†• Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Broadcasting Ø§Ø² ÙØ§ÛŒÙ„ 02
        broadcasting_sentiment_cols = [col for col in group.columns if 'sentiment' in col and any(x in col for x in ['compound_mean', 'positive_mean', 'negative_mean', 'neutral_mean'])]
        direct_sentiment_cols = [col for col in group.columns if col in ['sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral']]
        
        # Ø§Ú¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if direct_sentiment_cols:
            logging.debug("âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒØ§ÙØª Ø´Ø¯")
            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
            for col in ['sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral']:
                if col not in group.columns:
                    group[col] = 0
        elif broadcasting_sentiment_cols:
            logging.debug("âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Broadcasting Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒØ§ÙØª Ø´Ø¯ - Ø¯Ø± Ø­Ø§Ù„ Ù†Ú¯Ø§Ø´Øª...")
            # Ù†Ú¯Ø§Ø´Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Broadcasting Ø¨Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±
            sentiment_mapping = {
                'sentiment_compound_mean': 'sentiment_score',
                'sentiment_positive_mean': 'sentiment_positive',  
                'sentiment_negative_mean': 'sentiment_negative',
                'sentiment_neutral_mean': 'sentiment_neutral',
            }
            
            for broadcast_col, target_col in sentiment_mapping.items():
                if broadcast_col in group.columns:
                    group[target_col] = group[broadcast_col]
                    logging.debug(f"Ù†Ú¯Ø§Ø´Øª {broadcast_col} -> {target_col}")
                else:
                    group[target_col] = 0
        else:
            # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø³ØªÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†
            logging.debug("âš ï¸ Ù‡ÛŒÚ† Ø³ØªÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ - ØªÙ†Ø¸ÛŒÙ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶")
            for col in ['sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral']:
                group[col] = 0
                
        # ğŸ†• === Ù¾Ø±Ø¯Ø§Ø²Ø´ Telegram Features (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Reddit) ===
        telegram_features = ['telegram_prices', 'telegram_channel_type']
        for feature in telegram_features:
            if feature in group.columns:
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø¨Ø±Ø§ÛŒ Telegram features
                if feature == 'telegram_prices':
                    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numeric Ø§Ú¯Ø± Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ø¨Ø§Ø´Ø¯
                    group[f'{feature}_count'] = pd.to_numeric(group[feature], errors='coerce').fillna(0)
                    group[f'{feature}_ma'] = group[f'{feature}_count'].rolling(
                        window=INDICATOR_PARAMS['telegram_sentiment_ma'], min_periods=1
                    ).mean()
                    group[f'{feature}_momentum'] = group[f'{feature}_count'].diff(
                        INDICATOR_PARAMS['telegram_momentum_period']).fillna(0)
                
                logging.debug(f"Ù¾Ø±Ø¯Ø§Ø²Ø´ Telegram feature: {feature}")
            else:
                # Ø§Ú¯Ø± Telegram features Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³ØªØŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
                if feature == 'telegram_prices':
                    group[f'{feature}_count'] = 0
                    group[f'{feature}_ma'] = 0
                    group[f'{feature}_momentum'] = 0
        
        # === Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Reddit Features Ø¨Ø§ Telegram-based Features ===
        # Ø­Ø§Ù„Ø§ Ú©Ù‡ Ø§Ø² Telegram Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Reddit features Ø±Ø§ Ø¨Ø§ Telegram sentiment Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if 'sentiment_score' in group.columns:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² sentiment_score Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Reddit score Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† (Ø§Ø² Telegram Ù…ÛŒâ€ŒØ¢ÛŒØ¯)
            group['reddit_score'] = group['sentiment_score']  # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ
            group['reddit_comments'] = group['sentiment_score'] * 10  # ØªØ®Ù…ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù…Ù†Øª
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø¨Ø±Ø§ÛŒ "Reddit" features
            group['reddit_score_ma'] = group['reddit_score'].rolling(window=12, min_periods=1).mean()
            group['reddit_comments_ma'] = group['reddit_comments'].rolling(window=12, min_periods=1).mean()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ momentum Ø¨Ø±Ø§ÛŒ "Reddit" features
            group['reddit_score_momentum'] = group['reddit_score'].diff(12).fillna(0)
            group['reddit_comments_momentum'] = group['reddit_comments'].diff(12).fillna(0)
            
            logging.debug("âœ… Reddit features Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯Ù†Ø¯ Ø¨Ø§ Telegram-based features")
        else:
            # Ø§Ú¯Ø± sentiment_score ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            reddit_placeholder_features = ['reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma', 
                                         'reddit_score_momentum', 'reddit_comments_momentum']
            for feature in reddit_placeholder_features:
                group[feature] = 0
        
        # === Ù…Ø­Ø§Ø³Ø¨Ù‡ source diversity Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ ===
        if 'source_diversity' in group.columns:
            max_diversity = group['source_diversity'].max()
            group['source_diversity_normalized'] = group['source_diversity'] / max_diversity if max_diversity > 0 else 0
        else:
            group['source_diversity'] = 1
            group['source_diversity_normalized'] = 0
            
    except Exception as e:
        log_indicator_error('Sentiment and Telegram Features', group.name, e)
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        default_sentiment_features = [
            'sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral',
            'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
            'reddit_score_momentum', 'reddit_comments_momentum', 'source_diversity', 'source_diversity_normalized'
        ]
        for feature in default_sentiment_features:
            if feature not in group.columns:
                group[feature] = 0

    # === Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ ===
    if GLOBAL_COUNTER % 25 == 0:  # Ù‡Ø± 25 Ú¯Ø±ÙˆÙ‡ Ø¨Ù‡ Ø¬Ø§ÛŒ 50
        gc.collect()

    return group

def enhance_sentiment_features(df_features: pd.DataFrame, processed_data_path: str) -> pd.DataFrame:
    """
    ØªØ§Ø¨Ø¹ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª
    ğŸ†• Ø³Ø§Ø²Ú¯Ø§Ø± Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Broadcasting Ø¬Ø¯ÛŒØ¯ ÙØ§ÛŒÙ„ 02 Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
    """
    logging.info("ğŸ­ Ø´Ø±ÙˆØ¹ Ø¨Ù‡Ø¨ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ 02)...")
    
    try:
        # ğŸ†• Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…Ø®ØªÙ„Ù
        broadcasting_sentiment_cols = [col for col in df_features.columns if 'sentiment' in col and 'mean' in col]
        direct_sentiment_cols = [col for col in df_features.columns if col in ['sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral']]
        telegram_cols = [col for col in df_features.columns if 'telegram' in col]
        reddit_cols = [col for col in df_features.columns if 'reddit' in col]
        source_cols = [col for col in df_features.columns if 'source' in col]
        
        logging.info(f"âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Broadcasting sentiment ÛŒØ§ÙØª Ø´Ø¯Ù‡: {broadcasting_sentiment_cols}")
        logging.info(f"âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… sentiment ÛŒØ§ÙØª Ø´Ø¯Ù‡: {direct_sentiment_cols}")
        logging.info(f"âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Telegram ÛŒØ§ÙØª Ø´Ø¯Ù‡: {telegram_cols}")
        logging.info(f"âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Reddit ÛŒØ§ÙØª Ø´Ø¯Ù‡: {reddit_cols}")
        logging.info(f"âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Source ÛŒØ§ÙØª Ø´Ø¯Ù‡: {source_cols}")
        
        # ğŸ†• Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ: Ø§Ø¨ØªØ¯Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…ØŒ Ø³Ù¾Ø³ Broadcasting
        if direct_sentiment_cols:
            logging.info("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø­Ø³Ø§Ø³Ø§Øª")
            
            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ sentiment_score
            if 'sentiment_score' in df_features.columns:
                non_zero_sentiment = (df_features['sentiment_score'] != 0).sum()
                total_records = len(df_features)
                percentage = (non_zero_sentiment / total_records) * 100 if total_records > 0 else 0
                logging.info(f"ğŸ“Š Ø¢Ù…Ø§Ø± sentiment_score: {non_zero_sentiment:,} ØºÛŒØ±ØµÙØ± Ø§Ø² {total_records:,} ({percentage:.1f}%)")
            else:
                # Ø§Ú¯Ø± sentiment_score ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ Ø§Ù…Ø§ Ø³Ø§ÛŒØ± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ù‡Ø³ØªÙ†Ø¯
                df_features['sentiment_score'] = 0
                logging.warning("âš ï¸ sentiment_score ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ù…Ù‚Ø¯Ø§Ø± 0 ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")
                
        elif broadcasting_sentiment_cols:
            logging.info("âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Broadcasting Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ú¯Ø§Ø´Øª")
            
            # Ù†Ú¯Ø§Ø´Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Broadcasting
            sentiment_mapping = {
                'sentiment_compound_mean': 'sentiment_score',
                'sentiment_positive_mean': 'sentiment_positive',
                'sentiment_negative_mean': 'sentiment_negative',
                'sentiment_neutral_mean': 'sentiment_neutral'
            }
            
            for broadcast_col, target_col in sentiment_mapping.items():
                if broadcast_col in df_features.columns:
                    if target_col not in df_features.columns:
                        df_features[target_col] = df_features[broadcast_col]
                        logging.info(f"   âœ… Ù†Ú¯Ø§Ø´Øª: {broadcast_col} -> {target_col}")
                    else:
                        logging.info(f"   â„¹ï¸ {target_col} Ø§Ø² Ù‚Ø¨Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª")
                else:
                    if target_col not in df_features.columns:
                        df_features[target_col] = 0
                        logging.warning(f"   âš ï¸ {broadcast_col} ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ {target_col} = 0 ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")
            
            # Ø¢Ù…Ø§Ø± sentiment_score Ø¨Ø¹Ø¯ Ø§Ø² Ù†Ú¯Ø§Ø´Øª
            if 'sentiment_score' in df_features.columns:
                non_zero_sentiment = (df_features['sentiment_score'] != 0).sum()
                total_records = len(df_features)
                percentage = (non_zero_sentiment / total_records) * 100 if total_records > 0 else 0
                logging.info(f"ğŸ“Š Ø¢Ù…Ø§Ø± sentiment_score (Ø¨Ø¹Ø¯ Ø§Ø² Ù†Ú¯Ø§Ø´Øª): {non_zero_sentiment:,} ØºÛŒØ±ØµÙØ± Ø§Ø² {total_records:,} ({percentage:.1f}%)")
                
        else:
            logging.warning("âš ï¸ Ù‡ÛŒÚ† Ø³ØªÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ...")
            
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
            sentiment_files_patterns = [
                'master_merged_data_*.parquet',  # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯
                'sentiment_scores_raw_*.parquet',
                'sentiment_scores_daily_*.parquet', 
                'sentiment_scores_hourly_*.parquet'
            ]
            
            found_sentiment_file = None
            for pattern in sentiment_files_patterns:
                files = glob.glob(os.path.join(processed_data_path, pattern))
                if files:
                    found_sentiment_file = max(files, key=os.path.getctime)  # Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„
                    break
            
            if found_sentiment_file:
                logging.info(f"ğŸ“ ÙØ§ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ø§Ø±Ø¬ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {os.path.basename(found_sentiment_file)}")
                try:
                    sentiment_df = pd.read_parquet(found_sentiment_file)
                    logging.info(f"ğŸ“Š ÙØ§ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯: {sentiment_df.shape}")
                    
                    # Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙØ§ÛŒÙ„ Ø®Ø§Ø±Ø¬ÛŒ
                    external_broadcast_cols = [col for col in sentiment_df.columns if 'sentiment' in col and 'mean' in col]
                    external_direct_cols = [col for col in sentiment_df.columns if col in ['sentiment_score', 'sentiment_positive']]
                    
                    if external_broadcast_cols or external_direct_cols:
                        logging.info(f"âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø± ÙØ§ÛŒÙ„ Ø®Ø§Ø±Ø¬ÛŒ: {external_broadcast_cols + external_direct_cols}")
                        # Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ù†Ø·Ù‚ Ø§Ø¯ØºØ§Ù… Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯ Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ø¯
                    else:
                        logging.warning("âš ï¸ ÙØ§ÛŒÙ„ Ø®Ø§Ø±Ø¬ÛŒ Ù†ÛŒØ² ÙØ§Ù‚Ø¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø³Øª")
                        
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø®Ø§Ø±Ø¬ÛŒ: {e}")
            else:
                logging.warning("âš ï¸ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ø§Ø±Ø¬ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            
            # ØªÙ†Ø¸ÛŒÙ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            for col in ['sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral']:
                if col not in df_features.columns:
                    df_features[col] = 0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        logging.info("ğŸ§® Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        
        def calculate_advanced_sentiment_features(group):
            """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú¯Ø±ÙˆÙ‡"""
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
            if hasattr(group.index, 'get_level_values') and 'timestamp' in group.index.names:
                group = group.sort_index(level='timestamp')
            elif 'timestamp' in group.columns:
                group = group.sort_values('timestamp')
            else:
                group = group.sort_index()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ sentiment_momentum (ØªØºÛŒÛŒØ±Ø§Øª 24 Ø³Ø§Ø¹ØªÙ‡)
            momentum_period = min(INDICATOR_PARAMS['sentiment_momentum_period'], len(group))
            if momentum_period > 0:
                group['sentiment_momentum'] = group['sentiment_score'].diff(momentum_period).fillna(0)
            else:
                group['sentiment_momentum'] = 0
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø§Ø­Ø³Ø§Ø³Ø§Øª
            window_short = min(INDICATOR_PARAMS['sentiment_ma_short'] * 24, len(group))  # 7 Ø±ÙˆØ² * 24 Ø³Ø§Ø¹Øª
            window_long = min(INDICATOR_PARAMS['sentiment_ma_long'] * 24, len(group))   # 14 Ø±ÙˆØ² * 24 Ø³Ø§Ø¹Øª
            
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ú©ÙˆØªØ§Ù‡ Ù…Ø¯Øª
            if window_short > 0:
                group['sentiment_ma_7'] = group['sentiment_score'].rolling(
                    window=window_short, min_periods=1
                ).mean()
            else:
                group['sentiment_ma_7'] = group['sentiment_score']
            
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø¨Ù„Ù†Ø¯ Ù…Ø¯Øª
            if window_long > 0:
                group['sentiment_ma_14'] = group['sentiment_score'].rolling(
                    window=window_long, min_periods=1
                ).mean()
            else:
                group['sentiment_ma_14'] = group['sentiment_score']
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ sentiment_volume (ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª)
            if 'volume' in group.columns:
                # ØªØ±Ú©ÛŒØ¨ sentiment Ø¨Ø§ volume Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ sentiment_volume
                sentiment_abs = abs(group['sentiment_score'])
                volume_normalized = group['volume'] / group['volume'].max() if group['volume'].max() > 0 else 0
                group['sentiment_volume'] = sentiment_abs * volume_normalized
                group['sentiment_volume'] = group['sentiment_volume'].rolling(window=24, min_periods=1).sum()
            else:
                group['sentiment_volume'] = abs(group['sentiment_score']).rolling(window=24, min_periods=1).sum()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø² Ù‚ÛŒÙ…Øª (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)
            if 'close' in group.columns and len(group) > 20:
                try:
                    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‚ÛŒÙ…Øª Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª
                    price_returns = group['close'].pct_change(20).fillna(0)  # 20-period price change
                    sentiment_change = group['sentiment_score'].diff(20).fillna(0)  # 20-period sentiment change
                    
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ correlation rolling
                    correlation_window = min(50, len(group))
                    if correlation_window > 10:
                        rolling_corr = price_returns.rolling(window=correlation_window, min_periods=10).corr(sentiment_change)
                        group['sentiment_divergence'] = 1 - rolling_corr.fillna(0)  # ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ = 1 - Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
                    else:
                        group['sentiment_divergence'] = 1
                except:
                    group['sentiment_divergence'] = 1
            else:
                group['sentiment_divergence'] = 1
            
            # ğŸ†• === Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Telegram Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Reddit) ===
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² sentiment_score Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Telegram features
            if 'sentiment_score' in group.columns and group['sentiment_score'].sum() != 0:
                # "reddit_score" Ø¯Ø± ÙˆØ§Ù‚Ø¹ Ø§Ø² Telegram sentiment Ù…ÛŒâ€ŒØ¢ÛŒØ¯
                group['reddit_score'] = group['sentiment_score']
                group['reddit_comments'] = group['sentiment_score'] * 10  # ØªØ®Ù…ÛŒÙ†
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ momentum Ø¨Ø±Ø§ÛŒ Telegram-based "Reddit" features
                group['reddit_score_momentum'] = group['reddit_score'].diff(12).fillna(0)
                group['reddit_comments_momentum'] = group['reddit_comments'].diff(12).fillna(0)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú©
                group['reddit_score_ma'] = group['reddit_score'].rolling(window=12, min_periods=1).mean()
                group['reddit_comments_ma'] = group['reddit_comments'].rolling(window=12, min_periods=1).mean()
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ sentiment-reddit correlation (Ø¯Ø± ÙˆØ§Ù‚Ø¹ Ø®ÙˆØ¯Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ)
                if len(group) > 20:
                    corr_window = min(30, len(group))
                    # correlation Ø¨Ø§ Ø®ÙˆØ¯ sentiment (Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ)
                    group['sentiment_reddit_score_corr'] = 1.0  # Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ú©Ø§Ù…Ù„ Ú†ÙˆÙ† ÛŒÚ©Ø³Ø§Ù† Ù‡Ø³ØªÙ†Ø¯
                    group['sentiment_reddit_comments_corr'] = group['sentiment_score'].rolling(
                        window=corr_window, min_periods=10
                    ).corr(group['reddit_comments']).fillna(0.8)
                else:
                    group['sentiment_reddit_score_corr'] = 1.0
                    group['sentiment_reddit_comments_corr'] = 0.8
            else:
                # Ø§Ú¯Ø± sentiment_score Ø®Ø§Ù„ÛŒ Ø§Ø³Øª
                reddit_features = ['reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
                                 'reddit_score_momentum', 'reddit_comments_momentum',
                                 'sentiment_reddit_score_corr', 'sentiment_reddit_comments_corr']
                for feature in reddit_features:
                    group[feature] = 0
            
            # === Ù…Ø­Ø§Ø³Ø¨Ù‡ diversity features ===
            if 'source_diversity' in group.columns:
                max_diversity = group['source_diversity'].max()
                group['source_diversity_normalized'] = group['source_diversity'] / max_diversity if max_diversity > 0 else 0
                
                # ØªØ¹Ø§Ù…Ù„ diversity Ø¨Ø§ sentiment
                group['sentiment_diversity_interaction'] = group['sentiment_score'] * group['source_diversity_normalized']
            else:
                group['source_diversity_normalized'] = 0
                group['sentiment_diversity_interaction'] = 0
            
            # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± NaN
            sentiment_feature_columns = [col for col in group.columns if 'sentiment' in col or 'reddit' in col or 'source' in col]
            for col in sentiment_feature_columns:
                if col in group.columns:
                    group[col] = group[col].fillna(0)
            
            return group
        
        # Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ù‡ Ù‡Ø± Ú¯Ø±ÙˆÙ‡
        logging.info("ğŸ”„ Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        
        if isinstance(df_features.index, pd.MultiIndex):
            if 'symbol' in df_features.index.names and 'timeframe' in df_features.index.names:
                unique_groups = df_features.groupby(level=['symbol', 'timeframe']).ngroups
                logging.info(f"ğŸ”„ Ù¾Ø±Ø¯Ø§Ø²Ø´ {unique_groups} Ú¯Ø±ÙˆÙ‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
                
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² group_keys=False Ø¨Ø±Ø§ÛŒ Ø­Ù„ pandas deprecation warning
                df_features = df_features.groupby(level=['symbol', 'timeframe'], group_keys=False).apply(
                    calculate_advanced_sentiment_features
                )
            else:
                # Ø§Ú¯Ø± structure Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³ØªØŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†
                df_features = calculate_advanced_sentiment_features(df_features)
        else:
            # Ø§Ú¯Ø± MultiIndex Ù†ÛŒØ³Øª
            df_features = calculate_advanced_sentiment_features(df_features)
        
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        required_sentiment_features = [
            'sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 
            'sentiment_volume', 'sentiment_divergence',
            'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
            'reddit_score_momentum', 'reddit_comments_momentum',
            'sentiment_reddit_score_corr', 'sentiment_reddit_comments_corr',
            'source_diversity_normalized', 'sentiment_diversity_interaction'
        ]
        
        for feature in required_sentiment_features:
            if feature not in df_features.columns:
                df_features[feature] = 0
                logging.warning(f"âš ï¸ {feature} Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 0")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        logging.info("ğŸ“ˆ Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª:")
        for feature in required_sentiment_features[:6]:  # Ù†Ù…Ø§ÛŒØ´ 6 ÙˆÛŒÚ˜Ú¯ÛŒ Ø§ØµÙ„ÛŒ
            if feature in df_features.columns:
                stats = df_features[feature].describe()
                non_zero = (df_features[feature] != 0).sum()
                logging.info(f"   {feature}: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={stats['mean']:.4f}, Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={stats['std']:.4f}, ØºÛŒØ±ØµÙØ±={non_zero}")
        
        # ğŸ†• Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Telegram-based Reddit features
        telegram_based_reddit_features = ['reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma']
        reddit_stats = {}
        for feature in telegram_based_reddit_features:
            if feature in df_features.columns:
                non_zero = (df_features[feature] != 0).sum()
                reddit_stats[feature] = non_zero
        
        if any(reddit_stats.values()):
            logging.info("ğŸ“± Ø¢Ù…Ø§Ø± Telegram-based Reddit features:")
            for feature, count in reddit_stats.items():
                logging.info(f"   {feature}: {count} Ø±Ú©ÙˆØ±Ø¯ ØºÛŒØ±ØµÙØ±")
        else:
            logging.info("ğŸ“± Telegram-based Reddit features: Ù‡Ù…Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± ØµÙØ± (Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾Ø§ÛŒÙ‡ ØµÙØ± Ø§Ø³Øª)")
        
        logging.info("âœ… Ø¨Ù‡Ø¨ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡Ø¨ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª: {e}")
        import traceback
        logging.error(traceback.format_exc())
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        logging.info("ğŸ”„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        required_features = [
            'sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 
            'sentiment_volume', 'sentiment_divergence',
            'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
            'reddit_score_momentum', 'reddit_comments_momentum',
            'sentiment_reddit_score_corr', 'sentiment_reddit_comments_corr',
            'source_diversity_normalized', 'sentiment_diversity_interaction'
        ]
        for feature in required_features:
            if feature not in df_features.columns:
                df_features[feature] = 0
    
    return df_features

def validate_feature_count(df_features: pd.DataFrame) -> Tuple[int, List[str]]:
    """Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ‡"""
    exclude_cols = ['timestamp', 'symbol', 'timeframe', 'open', 'high', 'low', 'close', 'volume', 'target']
    
    if isinstance(df_features.index, pd.MultiIndex):
        # Ø§Ø² index names Ø­Ø°Ù Ú©Ù†
        index_cols = list(df_features.index.names) if df_features.index.names else []
        exclude_cols.extend(index_cols)
    
    feature_columns = [col for col in df_features.columns if col not in exclude_cols]
    
    # Ù„ÛŒØ³Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± (67 ÙˆÛŒÚ˜Ú¯ÛŒ - Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¯Ø± ØªØ¹Ø¯Ø§Ø¯)
    expected_features = [
        # Technical indicators (43 features)
        'rsi', 'macd', 'macd_hist', 'macd_signal',
        'bb_upper', 'bb_middle', 'bb_lower', 'bb_position',
        'atr', 'atr_percent', 'volatility', 'price_change',
        'vwap', 'vwap_deviation', 'obv', 'obv_change', 'mfi', 'ad',
        'stoch_k', 'stoch_d', 'williams_r', 'cci',
        'ema_short', 'ema_medium', 'ema_long', 'ema_short_above_medium', 'ema_medium_above_long',
        'ema_short_slope', 'ema_medium_slope',
        'sma_short', 'sma_medium', 'sma_long', 'price_above_sma_short', 'price_above_sma_medium', 'price_above_sma_long',
        'return_1', 'return_5', 'return_10', 'avg_return_5', 'avg_return_10',
        'hl_ratio', 'close_position', 'volume_ma', 'volume_ratio',
        'psar', 'price_above_psar', 'adx',
        
        # Sentiment features (16 features) - ğŸ†• Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡: sentiment_diversity_interaction
        'sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence',
        'reddit_score', 'reddit_comments', 'reddit_score_ma', 'reddit_comments_ma',
        'reddit_score_momentum', 'reddit_comments_momentum',
        'sentiment_reddit_score_corr', 'sentiment_reddit_comments_corr',
        'source_diversity_normalized', 'sentiment_diversity_interaction'
    ]
    
    missing_features = [f for f in expected_features if f not in feature_columns]
    
    return len(feature_columns), missing_features

def run_feature_engineering(input_path: str, output_path: str):
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ - Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡"""
    global GLOBAL_COUNTER, TOTAL_GROUPS
    
    start_time = datetime.now()
    logging.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ (Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ 01 Ùˆ 02 Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)...")
    logging.info(f"ğŸ“‹ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±: {INDICATOR_PARAMS}")
    
    # ÛŒØ§ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡
    list_of_files = glob.glob(os.path.join(input_path, 'master_*_data_*.parquet'))
    if not list_of_files:
        logging.error(f"âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ Ø¯Ø± Ù…Ø³ÛŒØ± '{input_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return
    latest_file = max(list_of_files, key=os.path.getctime)
    logging.info(f"ğŸ“‚ Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ: {os.path.basename(latest_file)}")
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡
    df = pd.read_parquet(latest_file)
    logging.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: {len(df):,}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„
    logging.info(f"ğŸ” Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„: Index={df.index.names}, Columns={list(df.columns)}")
    
    # ğŸ†• Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ø­Ø³Ø§Ø³Ø§Øª
    broadcasting_sentiment_cols = [col for col in df.columns if 'sentiment' in col and 'mean' in col]
    direct_sentiment_cols = [col for col in df.columns if col in ['sentiment_score', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral']]
    telegram_cols = [col for col in df.columns if 'telegram' in col]
    reddit_cols = [col for col in df.columns if 'reddit' in col]
    source_cols = [col for col in df.columns if 'source' in col]
    
    logging.info(f"ğŸ­ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Broadcasting sentiment Ù…ÙˆØ¬ÙˆØ¯: {broadcasting_sentiment_cols}")
    logging.info(f"ğŸ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… sentiment Ù…ÙˆØ¬ÙˆØ¯: {direct_sentiment_cols}")
    logging.info(f"ğŸ“± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Telegram Ù…ÙˆØ¬ÙˆØ¯: {telegram_cols}")
    logging.info(f"ğŸ”´ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Reddit Ù…ÙˆØ¬ÙˆØ¯: {reddit_cols}")
    logging.info(f"ğŸ“¡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Source Ù…ÙˆØ¬ÙˆØ¯: {source_cols}")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§
    if isinstance(df.index, pd.MultiIndex) and 'symbol' in df.index.names and 'timeframe' in df.index.names:
        TOTAL_GROUPS = df.groupby(level=['symbol', 'timeframe']).ngroups
    else:
        TOTAL_GROUPS = 1
    logging.info(f"ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´: {TOTAL_GROUPS:,}")
    
    # Ø§Ø¹Ù…Ø§Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    logging.info("âš™ï¸ Ø´Ø±ÙˆØ¹ Ø§Ø¹Ù…Ø§Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡...")
    logging.info(f"ğŸ’¡ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú¯Ø±ÙˆÙ‡: {INDICATOR_PARAMS['min_data_points']}")
    
    if isinstance(df.index, pd.MultiIndex) and 'symbol' in df.index.names and 'timeframe' in df.index.names:
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² group_keys=False Ø¨Ø±Ø§ÛŒ Ø­Ù„ pandas deprecation warning
        df_features = df.groupby(level=['symbol', 'timeframe'], group_keys=False).apply(apply_features)
    else:
        # Ø§Ú¯Ø± Ø³Ø§Ø®ØªØ§Ø± MultiIndex Ø¯Ø±Ø³Øª Ù†ÛŒØ³ØªØŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†
        logging.warning("âš ï¸ Ø³Ø§Ø®ØªØ§Ø± MultiIndex Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³ØªØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù„ Ø¯Ø§Ø¯Ù‡...")
        df_features = apply_features(df)
        if df_features is None:
            logging.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§")
            return
    
    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
    del df
    gc.collect()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù†ØªÛŒØ¬Ù‡
    if df_features is None or (isinstance(df_features, pd.DataFrame) and df_features.empty):
        logging.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§. Ø¹Ù…Ù„ÛŒØ§Øª Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
        return
    
    # Ø­Ø°Ù Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù†Ø¯Ø§Ø´ØªÙ† Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒØŒ None Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡â€ŒØ§Ù†Ø¯
    if isinstance(df_features, pd.DataFrame):
        initial_rows = len(df_features)
        df_features.dropna(how='all', inplace=True)
        final_rows_after_filter = len(df_features)
        logging.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {final_rows_after_filter:,} (Ø­Ø°Ù Ø´Ø¯Ù‡: {initial_rows - final_rows_after_filter:,})")
    
    # --- Ø¨Ø®Ø´ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡: Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª ---
    df_features = enhance_sentiment_features(df_features, input_path)

    logging.info("ğŸ¯ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ØªÙ…Ø§Ù… Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù...")
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ index ØµØ­ÛŒØ­
    if not isinstance(df_features.index, pd.MultiIndex):
        if 'symbol' in df_features.columns and 'timeframe' in df_features.columns and 'timestamp' in df_features.columns:
            df_features.set_index(['symbol', 'timeframe', 'timestamp'], inplace=True)
        else:
            logging.warning("âš ï¸ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† index ØµØ­ÛŒØ­ ØªÙ†Ø¸ÛŒÙ… Ú©Ø±Ø¯")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù Ø¨Ø§ Ø±ÙˆØ´ Ù¾ÙˆÛŒØ§
    logging.info(f"ğŸ”® Ù…Ø­Ø§Ø³Ø¨Ù‡ target Ø¨Ø§ {TARGET_FUTURE_PERIODS} Ù¾Ø±ÛŒÙˆØ¯ Ø¢ÛŒÙ†Ø¯Ù‡ Ùˆ {TARGET_PROFIT_PERCENT*100}% Ø³ÙˆØ¯")
    
    if isinstance(df_features.index, pd.MultiIndex) and 'symbol' in df_features.index.names and 'timeframe' in df_features.index.names:
        df_features['future_close'] = df_features.groupby(level=['symbol', 'timeframe'])['close'].shift(-TARGET_FUTURE_PERIODS)
    else:
        df_features['future_close'] = df_features['close'].shift(-TARGET_FUTURE_PERIODS)
    
    df_features['target'] = (df_features['future_close'] > df_features['close'] * (1 + TARGET_PROFIT_PERCENT)).astype(int)
    
    # Ø­Ø°Ù Ø³ØªÙˆÙ† Ú©Ù…Ú©ÛŒ Ùˆ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‚Øµ
    df_features.drop(columns=['future_close'], inplace=True)
    
    # === Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ===
    feature_count, missing_features = validate_feature_count(df_features)
    logging.info(f"ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡: {feature_count}")
    
    if missing_features:
        logging.warning(f"âš ï¸ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ‡ ({len(missing_features)}): {missing_features}")
    else:
        logging.info("âœ… Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯")
    
    # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø± NaN
    initial_rows = len(df_features)
    df_features.dropna(inplace=True)
    final_rows = len(df_features)
    logging.info(f"ğŸ§¹ ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ NaN: {initial_rows - final_rows:,}")
    
    logging.info(f"âœ… Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ {final_rows:,} Ø±Ø¯ÛŒÙ Ùˆ {feature_count} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.")
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (Ø¨Ø§ ØªØ§Ú©ÛŒØ¯ Ø¨Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª)
    logging.info("ğŸ“ˆ === Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ===")
    important_features = ['sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 
                         'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence',
                         'reddit_score', 'reddit_comments',
                         'rsi', 'macd', 'bb_position', 'atr_percent', 'volume_ratio']
    
    for col in important_features:
        if col in df_features.columns:
            mean_val = df_features[col].mean()
            std_val = df_features[col].std()
            non_zero = (df_features[col] != 0).sum()
            logging.info(f"   {col}: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={mean_val:.4f}, Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={std_val:.4f}, ØºÛŒØ±ØµÙØ±={non_zero}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ target
    target_distribution = df_features['target'].value_counts()
    logging.info(f"ğŸ¯ ØªÙˆØ²ÛŒØ¹ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù: {target_distribution.to_dict()}")
    target_percentage = (target_distribution.get(1, 0) / len(df_features)) * 100
    logging.info(f"ğŸ“Š Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª: {target_percentage:.2f}%")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ
    logging.info("ğŸ’¾ Ø´Ø±ÙˆØ¹ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ...")
    os.makedirs(output_path, exist_ok=True)
    timestamp_str = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ
    output_filename = f'final_dataset_for_training_{timestamp_str}.parquet'
    output_file_path = os.path.join(output_path, output_filename)
    df_features.to_parquet(output_file_path)
    logging.info(f"âœ… Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± Ù…Ø³ÛŒØ± '{output_file_path}' (ÙØ±Ù…Øª Parquet) Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ CSV Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªÛŒ
    csv_output_filename = f'final_dataset_for_training_{timestamp_str}.csv'
    csv_output_file_path = os.path.join(output_path, csv_output_filename)
    sample_size = min(1000, len(df_features))
    df_features.head(sample_size).to_csv(csv_output_file_path, index=True)
    logging.info(f"ğŸ“„ Ù†Ù…ÙˆÙ†Ù‡ CSV ({sample_size:,} Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„) Ø¯Ø± Ù…Ø³ÛŒØ± '{csv_output_file_path}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§
    end_time = datetime.now()
    execution_time = end_time - start_time
    logging.info(f"â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ú©Ù„: {execution_time}")
    
    # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n" + "="*80)
    print("ğŸ‰ === Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ (Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ + ÙØ§ÛŒÙ„ 02 Compatible) ===")
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {final_rows:,}")
    print(f"ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {feature_count}")
    print(f"ğŸ¯ Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª: {target_percentage:.2f}%")
    print(f"â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {execution_time}")
    print(f"ğŸ“ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: {output_filename}")
    print("\nğŸ†• ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Broadcasting + Multi-source + Telegram Compatible):")
    print("  âœ… sentiment_score (Ø§Ø² ÙØ§ÛŒÙ„ 02: sentiment_compound_mean ÛŒØ§ Ù…Ø³ØªÙ‚ÛŒÙ…)")
    print("  âœ… sentiment_momentum (ØªØºÛŒÛŒØ±Ø§Øª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡)")
    print("  âœ… sentiment_ma_7, sentiment_ma_14 (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú©)")
    print("  âœ… sentiment_volume (Ø­Ø¬Ù… ØªØ¹Ø§Ù…Ù„)")
    print("  âœ… sentiment_divergence (ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø§Ø² Ù‚ÛŒÙ…Øª)")
    print("\nğŸ“± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Telegram-based (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Reddit):")
    print("  âœ… reddit_score = sentiment_score (Ø§Ø² Telegram)")
    print("  âœ… reddit_comments = sentiment_score * 10 (ØªØ®Ù…ÛŒÙ†)")
    print("  âœ… reddit_score_ma, reddit_comments_ma (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú©)")
    print("  âœ… reddit_score_momentum, reddit_comments_momentum (ØªØ­Ø±Ú©)")
    print("  âœ… sentiment_reddit_*_corr (Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ)")
    print("\nğŸ“¡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Source Diversity:")
    print("  âœ… source_diversity_normalized (ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡)")
    print("  âœ… sentiment_diversity_interaction (ØªØ¹Ø§Ù…Ù„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ ØªÙ†ÙˆØ¹)")
    print("\nğŸ”§ Ø§ØµÙ„Ø§Ø­Ø§Øª ÙÙ†ÛŒ:")
    print("  âœ… ØªØ´Ø®ÛŒØµ ØµØ­ÛŒØ­ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„ 02")
    print("  âœ… Ù†Ú¯Ø§Ø´Øª sentiment_compound_mean -> sentiment_score")
    print("  âœ… Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Reddit Ø¨Ø§ Telegram-based features")
    print("  âœ… Ø±ÙØ¹ Ù…Ø´Ú©Ù„ PSAR missing")
    print("  âœ… Ø­Ù„ pandas deprecation warnings")
    print("  âœ… Ø¨Ù‡Ø¨ÙˆØ¯ MFI calculation")
    print("  âœ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ memory management")
    print("="*80)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    if final_rows > 0:
        print("\n--- Ù†Ù…ÙˆÙ†Ù‡ Ûµ Ø±Ø¯ÛŒÙ Ø¢Ø®Ø± Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ ---")
        display_cols = ['open', 'high', 'low', 'close', 'volume', 'target'] + \
                      [col for col in ['sentiment_score', 'reddit_score', 'rsi', 'macd', 'bb_position'] if col in df_features.columns][:5]
        print(df_features[display_cols].tail())
        
        print(f"\n--- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ ---")
        print(f"Shape: {df_features.shape}")
        print(f"Memory usage: {df_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # ğŸ†• Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± sentiment features Ø¨Ø§ ØªØ§Ú©ÛŒØ¯ Ø¨Ø± Ù†Ú¯Ø§Ø´Øª
        sentiment_stats = {}
        key_features = ['sentiment_score', 'reddit_score', 'reddit_comments']
        for col in key_features:
            if col in df_features.columns:
                non_zero = (df_features[col] != 0).sum()
                sentiment_stats[col] = non_zero
        
        if sentiment_stats:
            print(f"\n--- Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Telegram-based) ---")
            for col, count in sentiment_stats.items():
                percentage = (count / len(df_features)) * 100
                print(f"{col}: {count:,} ØºÛŒØ±ØµÙØ± ({percentage:.1f}%)")
                
            # Ù†Ù…Ø§ÛŒØ´ Ù…ÙˆÙÙ‚ÛŒØª Ù†Ú¯Ø§Ø´Øª
            if sentiment_stats.get('sentiment_score', 0) > 0:
                print("âœ… Ù†Ú¯Ø§Ø´Øª Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„ 02 Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
                if sentiment_stats.get('reddit_score', 0) > 0:
                    print("âœ… Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Reddit Ø¨Ø§ Telegram-based features Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
            else:
                print("âš ï¸ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù‡Ù…Ú†Ù†Ø§Ù† ØµÙØ± - Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ±")

if __name__ == '__main__':
    run_feature_engineering(PROCESSED_DATA_PATH, FEATURES_PATH)