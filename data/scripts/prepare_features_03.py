#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ (ÙØ§Ø² Û³ØŒ Ú¯Ø§Ù… Ø§Ù„Ù) - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡

ØªØºÛŒÛŒØ±Ø§Øª Ø§ØµÙ„ÛŒ:
- Ø§ØµÙ„Ø§Ø­ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ sentiment Ø¬Ø¯ÛŒØ¯ (symbol-level aggregation)
- Ø­Ù„ Ù…Ø´Ú©Ù„ KeyError Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ sentiment
- Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Broadcasting sentiment
- Ù…Ø­Ø§Ø³Ø¨Ù‡ ØµØ­ÛŒØ­ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ sentiment Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
- Ø¨Ù‡Ø¨ÙˆØ¯ error handling Ùˆ fallback mechanisms
"""
import os
import glob
import pandas as pd
import pandas_ta as ta
import logging
import configparser
from typing import Optional, Dict, Any
import numpy as np
import gc
from datetime import datetime

# Ø¨Ø®Ø´ Ø®ÙˆØ§Ù†Ø¯Ù† Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
config = configparser.ConfigParser()
CONFIG_FILE_PATH = 'config.ini'
try:
    config.read(CONFIG_FILE_PATH, encoding='utf-8')
    PROCESSED_DATA_PATH = config.get('Paths', 'processed')
    FEATURES_PATH = config.get('Paths', 'features')
    LOG_PATH = config.get('Paths', 'logs')
except Exception as e:
    print(f"CRITICAL ERROR: Could not read 'config.ini'. Error: {e}")
    exit()

# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)
os.makedirs(FEATURES_PATH, exist_ok=True)
os.makedirs(LOG_PATH, exist_ok=True)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ù¾ÙˆÛŒØ§ ---
script_name = os.path.splitext(os.path.basename(__file__))[0]
log_subfolder_path = os.path.join(LOG_PATH, script_name)
os.makedirs(log_subfolder_path, exist_ok=True)

log_filename = os.path.join(log_subfolder_path, f"log_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])

# === Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾ÙˆÛŒØ§ Ùˆ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ… ===
INDICATOR_PARAMS = {
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    'rsi_length': 14,
    'macd_fast': 12,
    'macd_slow': 26,
    'macd_signal': 9,
    'bb_length': 20,
    'bb_std': 2.0,
    
    # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
    'atr_length': 14,
    'vwap_anchor': None,  # None Ø¨Ø±Ø§ÛŒ VWAP Ø±ÙˆØ²Ø§Ù†Ù‡
    
    # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù† Ùˆ ØªØ±Ù†Ø¯
    'stoch_k': 14,
    'stoch_d': 3,
    'stoch_smooth': 3,
    'williams_r_length': 14,
    'cci_length': 20,
    
    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªØ­Ø±Ú©
    'ema_short': 12,
    'ema_medium': 26,
    'ema_long': 50,
    'sma_short': 10,
    'sma_medium': 20,
    'sma_long': 50,
    
    # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø­Ø¬Ù…
    'obv_enabled': True,
    'mfi_length': 14,
    'ad_enabled': True,
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¬Ø¯ÛŒØ¯
    'sentiment_ma_short': 7,
    'sentiment_ma_long': 14,
    'sentiment_momentum_period': 24,  # 24 Ø³Ø§Ø¹Øª
    
    # Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
    'min_data_points': 100,
}

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ùˆ Ù‡Ø¯Ù
TARGET_FUTURE_PERIODS = 24
TARGET_PROFIT_PERCENT = 0.02

# Ú©Ø§Ù†ØªØ± global Ø¨Ø±Ø§ÛŒ tracking
GLOBAL_COUNTER = 0
TOTAL_GROUPS = 0

def log_indicator_error(indicator_name: str, group_name: Any, error: Exception):
    """ØªØ§Ø¨Ø¹ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§"""
    logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ {indicator_name} Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙˆÙ‡ {group_name}: {error}")

def log_progress(current: int, total: int, group_name: str = ""):
    """Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª Ù¾Ø±Ø¯Ø§Ø²Ø´"""
    if total > 0:
        progress = (current / total) * 100
        if current % max(1, total // 20) == 0:  # Ù‡Ø± 5% Ú¯Ø²Ø§Ø±Ø´
            logging.info(f"ğŸ”„ Ù¾ÛŒØ´Ø±ÙØª: {progress:.1f}% ({current}/{total}) - {group_name}")

def apply_features(group: pd.DataFrame) -> Optional[pd.DataFrame]:
    """
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ØªÙ…Ø§Ù… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú¯Ø±ÙˆÙ‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    global GLOBAL_COUNTER, TOTAL_GROUPS
    GLOBAL_COUNTER += 1
    
    # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
    log_progress(GLOBAL_COUNTER, TOTAL_GROUPS, str(group.name))
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
    if len(group) < INDICATOR_PARAMS['min_data_points']:
        logging.debug(f"Ú¯Ø±ÙˆÙ‡ {group.name} Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±Ø¯ ({len(group)} < {INDICATOR_PARAMS['min_data_points']})")
        return None

    # === Ø¨Ø®Ø´ Û±: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªØ±Ù†Ø¯ Ùˆ Ù‚ÛŒÙ…Øª ===
    try:
        group['rsi'] = ta.rsi(group['close'], length=INDICATOR_PARAMS['rsi_length'])
    except Exception as e:
        log_indicator_error('RSI', group.name, e)

    try:
        macd = ta.macd(group['close'], 
                      fast=INDICATOR_PARAMS['macd_fast'], 
                      slow=INDICATOR_PARAMS['macd_slow'], 
                      signal=INDICATOR_PARAMS['macd_signal'])
        if macd is not None and not macd.empty:
            group['macd'] = macd[f'MACD_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
            group['macd_hist'] = macd[f'MACDh_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
            group['macd_signal'] = macd[f'MACDs_{INDICATOR_PARAMS["macd_fast"]}_{INDICATOR_PARAMS["macd_slow"]}_{INDICATOR_PARAMS["macd_signal"]}']
    except Exception as e:
        log_indicator_error('MACD', group.name, e)

    try:
        bbands = ta.bbands(group['close'], 
                          length=INDICATOR_PARAMS['bb_length'], 
                          std=INDICATOR_PARAMS['bb_std'])
        if bbands is not None and not bbands.empty:
            group['bb_upper'] = bbands[f'BBU_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            group['bb_middle'] = bbands[f'BBM_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            group['bb_lower'] = bbands[f'BBL_{INDICATOR_PARAMS["bb_length"]}_{INDICATOR_PARAMS["bb_std"]}']
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚ÛŒÙ…Øª Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ Bollinger Bands
            group['bb_position'] = (group['close'] - group['bb_lower']) / (group['bb_upper'] - group['bb_lower'])
    except Exception as e:
        log_indicator_error('Bollinger Bands', group.name, e)

    # === Ø¨Ø®Ø´ Û²: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù† (Volatility) ===
    try:
        group['atr'] = ta.atr(group['high'], group['low'], group['close'], 
                             length=INDICATOR_PARAMS['atr_length'])
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡ (ATR Ø¨Ù‡ Ù†Ø³Ø¨Øª Ù‚ÛŒÙ…Øª)
        group['atr_percent'] = (group['atr'] / group['close']) * 100
    except Exception as e:
        log_indicator_error('ATR', group.name, e)

    try:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†ÙˆØ³Ø§Ù† ØªØ§Ø±ÛŒØ®ÛŒ (Historical Volatility)
        group['price_change'] = group['close'].pct_change()
        group['volatility'] = group['price_change'].rolling(window=20).std() * 100
    except Exception as e:
        log_indicator_error('Historical Volatility', group.name, e)

    # === Ø¨Ø®Ø´ Û³: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø­Ø¬Ù… (Volume-Based) ===
    try:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ VWAP Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ MultiIndex
        typical_price = (group['high'] + group['low'] + group['close']) / 3
        vwap_numerator = (typical_price * group['volume']).cumsum()
        vwap_denominator = group['volume'].cumsum()
        group['vwap'] = vwap_numerator / vwap_denominator
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø­Ø±Ø§Ù Ù‚ÛŒÙ…Øª Ø§Ø² VWAP
        group['vwap_deviation'] = ((group['close'] - group['vwap']) / group['vwap']) * 100
    except Exception as e:
        log_indicator_error('VWAP', group.name, e)

    if INDICATOR_PARAMS['obv_enabled']:
        try:
            group['obv'] = ta.obv(group['close'], group['volume'])
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª OBV
            group['obv_change'] = group['obv'].pct_change()
        except Exception as e:
            log_indicator_error('OBV', group.name, e)

    try:
        group['mfi'] = ta.mfi(group['high'], group['low'], group['close'], group['volume'], 
                             length=INDICATOR_PARAMS['mfi_length'])
    except Exception as e:
        log_indicator_error('MFI', group.name, e)

    if INDICATOR_PARAMS['ad_enabled']:
        try:
            group['ad'] = ta.ad(group['high'], group['low'], group['close'], group['volume'])
        except Exception as e:
            log_indicator_error('A/D Line', group.name, e)

    # === Ø¨Ø®Ø´ Û´: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§ÙˆØ³ÛŒÙ„Ø§ØªÙˆØ± ===
    try:
        stoch = ta.stoch(group['high'], group['low'], group['close'], 
                        k=INDICATOR_PARAMS['stoch_k'], 
                        d=INDICATOR_PARAMS['stoch_d'], 
                        smooth_k=INDICATOR_PARAMS['stoch_smooth'])
        if stoch is not None and not stoch.empty:
            group['stoch_k'] = stoch[f'STOCHk_{INDICATOR_PARAMS["stoch_k"]}_{INDICATOR_PARAMS["stoch_d"]}_{INDICATOR_PARAMS["stoch_smooth"]}']
            group['stoch_d'] = stoch[f'STOCHd_{INDICATOR_PARAMS["stoch_k"]}_{INDICATOR_PARAMS["stoch_d"]}_{INDICATOR_PARAMS["stoch_smooth"]}']
    except Exception as e:
        log_indicator_error('Stochastic', group.name, e)

    try:
        group['williams_r'] = ta.willr(group['high'], group['low'], group['close'], 
                                      length=INDICATOR_PARAMS['williams_r_length'])
    except Exception as e:
        log_indicator_error('Williams %R', group.name, e)

    try:
        group['cci'] = ta.cci(group['high'], group['low'], group['close'], 
                             length=INDICATOR_PARAMS['cci_length'])
    except Exception as e:
        log_indicator_error('CCI', group.name, e)

    # === Ø¨Ø®Ø´ Ûµ: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªØ­Ø±Ú© ===
    try:
        group['ema_short'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_short'])
        group['ema_medium'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_medium'])
        group['ema_long'] = ta.ema(group['close'], length=INDICATOR_PARAMS['ema_long'])
        
        # Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ cross over
        group['ema_short_above_medium'] = (group['ema_short'] > group['ema_medium']).astype(int)
        group['ema_medium_above_long'] = (group['ema_medium'] > group['ema_long']).astype(int)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÛŒØ¨ EMA (ØªØ±Ù†Ø¯)
        group['ema_short_slope'] = group['ema_short'].pct_change(periods=5)
        group['ema_medium_slope'] = group['ema_medium'].pct_change(periods=5)
    except Exception as e:
        log_indicator_error('EMA', group.name, e)

    try:
        group['sma_short'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_short'])
        group['sma_medium'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_medium'])
        group['sma_long'] = ta.sma(group['close'], length=INDICATOR_PARAMS['sma_long'])
        
        # Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚ÛŒÙ…Øª Ù†Ø³Ø¨Øª Ø¨Ù‡ SMA
        group['price_above_sma_short'] = (group['close'] > group['sma_short']).astype(int)
        group['price_above_sma_medium'] = (group['close'] > group['sma_medium']).astype(int)
        group['price_above_sma_long'] = (group['close'] > group['sma_long']).astype(int)
    except Exception as e:
        log_indicator_error('SMA', group.name, e)

    # === Ø¨Ø®Ø´ Û¶: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…Øª Ø®Ø§Ù… ===
    try:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        group['return_1'] = group['close'].pct_change(1)
        group['return_5'] = group['close'].pct_change(5)
        group['return_10'] = group['close'].pct_change(10)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø§Ø²Ø¯Ù‡
        group['avg_return_5'] = group['return_1'].rolling(window=5).mean()
        group['avg_return_10'] = group['return_1'].rolling(window=10).mean()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ High-Low ratio
        group['hl_ratio'] = (group['high'] - group['low']) / group['close']
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª close Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ high-low
        group['close_position'] = (group['close'] - group['low']) / (group['high'] - group['low'])
        
        # Ø­Ø¬Ù… Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡
        group['volume_ma'] = group['volume'].rolling(window=20).mean()
        group['volume_ratio'] = group['volume'] / group['volume_ma']
        
    except Exception as e:
        log_indicator_error('Price Features', group.name, e)

    # === Ø¨Ø®Ø´ Û·: Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ) ===
    try:
        # Parabolic SAR Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ù†ØªÛŒØ¬Ù‡
        psar_result = ta.psar(group['high'], group['low'], group['close'])
        if psar_result is not None:
            if isinstance(psar_result, pd.DataFrame):
                # Ø§Ú¯Ø± DataFrame Ø§Ø³ØªØŒ Ø³ØªÙˆÙ† Ø§ÙˆÙ„ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                group['psar'] = psar_result.iloc[:, 0]
            else:
                # Ø§Ú¯Ø± Series Ø§Ø³Øª
                group['psar'] = psar_result
            group['price_above_psar'] = (group['close'] > group['psar']).astype(int)
    except Exception as e:
        log_indicator_error('Parabolic SAR', group.name, e)

    try:
        # ADX Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ù†ØªÛŒØ¬Ù‡
        adx_result = ta.adx(group['high'], group['low'], group['close'], length=14)
        if adx_result is not None:
            if isinstance(adx_result, pd.DataFrame):
                # Ø§Ú¯Ø± DataFrame Ø§Ø³ØªØŒ Ø³ØªÙˆÙ† ADX Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                if 'ADX_14' in adx_result.columns:
                    group['adx'] = adx_result['ADX_14']
                else:
                    group['adx'] = adx_result.iloc[:, 0]  # Ø³ØªÙˆÙ† Ø§ÙˆÙ„
            else:
                # Ø§Ú¯Ø± Series Ø§Ø³Øª
                group['adx'] = adx_result

    except Exception as e:
        log_indicator_error('ADX', group.name, e)

    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
    if GLOBAL_COUNTER % 50 == 0:  # Ù‡Ø± 50 Ú¯Ø±ÙˆÙ‡
        gc.collect()

    return group

def enhance_sentiment_features(df_features: pd.DataFrame, processed_data_path: str) -> pd.DataFrame:
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª
    Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¬Ø¯ÛŒØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ sentiment (Broadcasting)
    """
    logging.info("ğŸ­ Ø´Ø±ÙˆØ¹ Ø¨Ù‡Ø¨ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡)...")
    
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø± ÙØ§ÛŒÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
        existing_sentiment_cols = [col for col in df_features.columns if 'sentiment' in col]
        
        if existing_sentiment_cols:
            logging.info(f"âœ… Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø² Ù‚Ø¨Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª: {existing_sentiment_cols}")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ sentiment_score Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
            if 'sentiment_compound_mean' in df_features.columns:
                df_features['sentiment_score'] = df_features['sentiment_compound_mean']
                logging.info("âœ… sentiment_score Ø§Ø² sentiment_compound_mean Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
            else:
                df_features['sentiment_score'] = 0
                logging.warning("âš ï¸ sentiment_compound_mean ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ sentiment_score = 0 ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")
        else:
            logging.warning("âš ï¸ Ù‡ÛŒÚ† Ø³ØªÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø² ÙØ§ÛŒÙ„...")
            
            # Ø¬Ø³ØªØ¬Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± Ù…Ø³ÛŒØ± processed Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
            sentiment_raw_files = glob.glob(os.path.join(PROCESSED_DATA_PATH, 'sentiment_scores_raw_*.parquet'))
            sentiment_daily_files = glob.glob(os.path.join(PROCESSED_DATA_PATH, 'sentiment_scores_daily_*.parquet'))
            sentiment_hourly_files = glob.glob(os.path.join(PROCESSED_DATA_PATH, 'sentiment_scores_hourly_*.parquet'))
            
            logging.info(f"ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒØ§ÙØª Ø´Ø¯Ù‡: Raw={len(sentiment_raw_files)}, Daily={len(sentiment_daily_files)}, Hourly={len(sentiment_hourly_files)}")
            
            if not (sentiment_raw_files or sentiment_daily_files or sentiment_hourly_files):
                logging.warning("âš ï¸ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶.")
                df_features['sentiment_score'] = 0
            else:
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ø§Ø¯ØºØ§Ù… Ø§Ø­Ø³Ø§Ø³Ø§Øª
                logging.info("ğŸ”„ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
                df_features['sentiment_score'] = 0  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
                
                # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ hourly Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
                if sentiment_hourly_files:
                    try:
                        latest_hourly_file = max(sentiment_hourly_files, key=os.path.getctime)
                        logging.info(f"ğŸ“Š Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø³Ø§Ø¹ØªÛŒ: {os.path.basename(latest_hourly_file)}")
                        sentiment_hourly_df = pd.read_parquet(latest_hourly_file)
                        
                        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø¯ØºØ§Ù… Ø³Ø§Ø¯Ù‡
                        if 'symbol' in sentiment_hourly_df.columns and 'sentiment_compound_mean' in sentiment_hourly_df.columns:
                            # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ symbol Ùˆ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ
                            symbol_sentiment = sentiment_hourly_df.groupby('symbol')['sentiment_compound_mean'].mean().to_dict()
                            
                            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ† symbol Ø¯Ø± df_features
                            if df_features.index.names and 'symbol' in df_features.index.names:
                                df_features = df_features.reset_index()
                                df_features['sentiment_score'] = df_features['symbol'].map(symbol_sentiment).fillna(0)
                                df_features.set_index(['symbol', 'timeframe', 'timestamp'], inplace=True)
                                logging.info("âœ… Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¯ØºØ§Ù… Ø´Ø¯")
                            else:
                                logging.warning("âš ï¸ Ø³Ø§Ø®ØªØ§Ø± index Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³Øª")
                    except Exception as e:
                        logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ hourly: {e}")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ sentiment_score
        logging.info("ğŸ§® Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        
        def calculate_advanced_sentiment_features(group):
            """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú¯Ø±ÙˆÙ‡"""
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
            group = group.sort_values('timestamp') if 'timestamp' in group.columns else group.sort_index()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ sentiment_momentum (ØªØºÛŒÛŒØ±Ø§Øª 24 Ø³Ø§Ø¹ØªÙ‡)
            if len(group) >= INDICATOR_PARAMS['sentiment_momentum_period']:
                group['sentiment_momentum'] = group['sentiment_score'].diff(
                    INDICATOR_PARAMS['sentiment_momentum_period']
                )
            else:
                group['sentiment_momentum'] = 0
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø§Ø­Ø³Ø§Ø³Ø§Øª
            window_short = min(INDICATOR_PARAMS['sentiment_ma_short'] * 24, len(group))  # 7 Ø±ÙˆØ² * 24 Ø³Ø§Ø¹Øª
            window_long = min(INDICATOR_PARAMS['sentiment_ma_long'] * 24, len(group))   # 14 Ø±ÙˆØ² * 24 Ø³Ø§Ø¹Øª
            
            if window_short > 0:
                group['sentiment_ma_7'] = group['sentiment_score'].rolling(
                    window=window_short, min_periods=1
                ).mean()
            else:
                group['sentiment_ma_7'] = group['sentiment_score']
            
            if window_long > 0:
                group['sentiment_ma_14'] = group['sentiment_score'].rolling(
                    window=window_long, min_periods=1
                ).mean()
            else:
                group['sentiment_ma_14'] = group['sentiment_score']
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ sentiment_volume (ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± - Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)
            # Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªØŒ Ø§Ø² ÛŒÚ© metric ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            group['sentiment_volume'] = abs(group['sentiment_score']).rolling(window=24, min_periods=1).sum()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø² Ù‚ÛŒÙ…Øª
            if 'close' in group.columns and group['sentiment_score'].std() > 0:
                try:
                    price_normalized = (group['close'] - group['close'].mean()) / group['close'].std()
                    sentiment_normalized = (group['sentiment_score'] - group['sentiment_score'].mean()) / group['sentiment_score'].std()
                    group['sentiment_divergence'] = price_normalized - sentiment_normalized
                except:
                    group['sentiment_divergence'] = 0
            else:
                group['sentiment_divergence'] = 0
            
            # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± NaN
            for col in ['sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence']:
                if col in group.columns:
                    group[col] = group[col].fillna(0)
            
            return group
        
        # Ø§Ø¹Ù…Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ù‡ Ù‡Ø± Ú¯Ø±ÙˆÙ‡
        if isinstance(df_features.index, pd.MultiIndex):
            if 'symbol' in df_features.index.names and 'timeframe' in df_features.index.names:
                df_features = df_features.reset_index()
                unique_groups = df_features.groupby(['symbol', 'timeframe']).ngroups
                logging.info(f"ğŸ”„ Ù¾Ø±Ø¯Ø§Ø²Ø´ {unique_groups} Ú¯Ø±ÙˆÙ‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
                
                df_features = df_features.groupby(['symbol', 'timeframe']).apply(
                    calculate_advanced_sentiment_features
                ).reset_index(drop=True)
                
                # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† index
                df_features.set_index(['symbol', 'timeframe', 'timestamp'], inplace=True)
            else:
                # Ø§Ú¯Ø± structure Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³ØªØŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†
                df_features = calculate_advanced_sentiment_features(df_features)
        else:
            # Ø§Ú¯Ø± MultiIndex Ù†ÛŒØ³Øª
            df_features = calculate_advanced_sentiment_features(df_features)
        
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        required_sentiment_features = ['sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence']
        for feature in required_sentiment_features:
            if feature not in df_features.columns:
                df_features[feature] = 0
                logging.warning(f"âš ï¸ {feature} Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 0")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        logging.info("ğŸ“ˆ Ø¢Ù…Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª:")
        for feature in required_sentiment_features:
            if feature in df_features.columns:
                stats = df_features[feature].describe()
                non_zero = (df_features[feature] != 0).sum()
                logging.info(f"   {feature}: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={stats['mean']:.4f}, Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={stats['std']:.4f}, ØºÛŒØ±ØµÙØ±={non_zero}")
        
        logging.info("âœ… Ø¨Ù‡Ø¨ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡Ø¨ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª: {e}")
        import traceback
        logging.error(traceback.format_exc())
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        logging.info("ğŸ”„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø­Ø³Ø§Ø³Ø§Øª...")
        required_features = ['sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence']
        for feature in required_features:
            if feature not in df_features.columns:
                df_features[feature] = 0
    
    return df_features

def run_feature_engineering(input_path: str, output_path: str):
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ"""
    global GLOBAL_COUNTER, TOTAL_GROUPS
    
    start_time = datetime.now()
    logging.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø­Ø³Ø§Ø³Ø§Øª Broadcasting)...")
    logging.info(f"ğŸ“‹ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±: {INDICATOR_PARAMS}")
    
    # ÛŒØ§ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡
    list_of_files = glob.glob(os.path.join(input_path, 'master_*_data_*.parquet'))
    if not list_of_files:
        logging.error(f"âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ Ø¯Ø± Ù…Ø³ÛŒØ± '{input_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return
    latest_file = max(list_of_files, key=os.path.getctime)
    logging.info(f"ğŸ“‚ Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ: {os.path.basename(latest_file)}")
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡
    df = pd.read_parquet(latest_file)
    logging.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: {len(df):,}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„
    logging.info(f"ğŸ” Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„: Index={df.index.names}, Columns={list(df.columns)}")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§
    if isinstance(df.index, pd.MultiIndex) and 'symbol' in df.index.names and 'timeframe' in df.index.names:
        TOTAL_GROUPS = df.groupby(level=['symbol', 'timeframe']).ngroups
    else:
        TOTAL_GROUPS = 1
    logging.info(f"ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´: {TOTAL_GROUPS:,}")
    
    # Ø§Ø¹Ù…Ø§Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    logging.info("âš™ï¸ Ø´Ø±ÙˆØ¹ Ø§Ø¹Ù…Ø§Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡...")
    logging.info(f"ğŸ’¡ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú¯Ø±ÙˆÙ‡: {INDICATOR_PARAMS['min_data_points']}")
    
    if isinstance(df.index, pd.MultiIndex) and 'symbol' in df.index.names and 'timeframe' in df.index.names:
        df_features = df.groupby(level=['symbol', 'timeframe'], group_keys=False).apply(apply_features)
    else:
        # Ø§Ú¯Ø± Ø³Ø§Ø®ØªØ§Ø± MultiIndex Ø¯Ø±Ø³Øª Ù†ÛŒØ³ØªØŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†
        logging.warning("âš ï¸ Ø³Ø§Ø®ØªØ§Ø± MultiIndex Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³ØªØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù„ Ø¯Ø§Ø¯Ù‡...")
        df_features = apply_features(df)
        if df_features is None:
            logging.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§")
            return
        df_features = pd.DataFrame([df_features])  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame
    
    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
    del df
    gc.collect()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù†ØªÛŒØ¬Ù‡
    if df_features is None or (isinstance(df_features, pd.DataFrame) and df_features.empty):
        logging.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§. Ø¹Ù…Ù„ÛŒØ§Øª Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
        return
    
    # Ø­Ø°Ù Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù†Ø¯Ø§Ø´ØªÙ† Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒØŒ None Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡â€ŒØ§Ù†Ø¯
    if isinstance(df_features, pd.DataFrame):
        initial_rows = len(df_features)
        df_features.dropna(how='all', inplace=True)
        final_rows_after_filter = len(df_features)
        logging.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {final_rows_after_filter:,} (Ø­Ø°Ù Ø´Ø¯Ù‡: {initial_rows - final_rows_after_filter:,})")
    
    # --- Ø¨Ø®Ø´ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡: Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª ---
    df_features = enhance_sentiment_features(df_features, input_path)

    logging.info("ğŸ¯ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ØªÙ…Ø§Ù… Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù...")
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ index ØµØ­ÛŒØ­
    if not isinstance(df_features.index, pd.MultiIndex):
        if 'symbol' in df_features.columns and 'timeframe' in df_features.columns and 'timestamp' in df_features.columns:
            df_features.set_index(['symbol', 'timeframe', 'timestamp'], inplace=True)
        else:
            logging.warning("âš ï¸ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† index ØµØ­ÛŒØ­ ØªÙ†Ø¸ÛŒÙ… Ú©Ø±Ø¯")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù Ø¨Ø§ Ø±ÙˆØ´ Ù¾ÙˆÛŒØ§
    logging.info(f"ğŸ”® Ù…Ø­Ø§Ø³Ø¨Ù‡ target Ø¨Ø§ {TARGET_FUTURE_PERIODS} Ù¾Ø±ÛŒÙˆØ¯ Ø¢ÛŒÙ†Ø¯Ù‡ Ùˆ {TARGET_PROFIT_PERCENT*100}% Ø³ÙˆØ¯")
    
    if isinstance(df_features.index, pd.MultiIndex) and 'symbol' in df_features.index.names and 'timeframe' in df_features.index.names:
        df_features['future_close'] = df_features.groupby(level=['symbol', 'timeframe'])['close'].shift(-TARGET_FUTURE_PERIODS)
    else:
        df_features['future_close'] = df_features['close'].shift(-TARGET_FUTURE_PERIODS)
    
    df_features['target'] = (df_features['future_close'] > df_features['close'] * (1 + TARGET_PROFIT_PERCENT)).astype(int)
    
    # Ø­Ø°Ù Ø³ØªÙˆÙ† Ú©Ù…Ú©ÛŒ Ùˆ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‚Øµ
    df_features.drop(columns=['future_close'], inplace=True)
    
    # Ø´Ù…Ø§Ø±Ø´ Ú©Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    exclude_cols = ['timestamp', 'symbol', 'timeframe', 'open', 'high', 'low', 'close', 'volume', 'target']
    if isinstance(df_features.index, pd.MultiIndex):
        feature_columns = [col for col in df_features.columns if col not in exclude_cols]
    else:
        feature_columns = [col for col in df_features.columns if col not in exclude_cols]
    
    logging.info(f"ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡: {len(feature_columns)}")
    
    # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø± NaN
    initial_rows = len(df_features)
    df_features.dropna(inplace=True)
    final_rows = len(df_features)
    logging.info(f"ğŸ§¹ ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ NaN: {initial_rows - final_rows:,}")
    
    logging.info(f"âœ… Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ {final_rows:,} Ø±Ø¯ÛŒÙ Ùˆ {len(feature_columns)} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.")
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (Ø¨Ø§ ØªØ§Ú©ÛŒØ¯ Ø¨Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª)
    logging.info("ğŸ“ˆ === Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ===")
    important_features = ['sentiment_score', 'sentiment_momentum', 'sentiment_ma_7', 
                         'sentiment_ma_14', 'sentiment_volume', 'sentiment_divergence',
                         'rsi', 'macd', 'bb_position', 'atr_percent', 'volume_ratio']
    
    for col in important_features:
        if col in df_features.columns:
            mean_val = df_features[col].mean()
            std_val = df_features[col].std()
            logging.info(f"   {col}: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={mean_val:.4f}, Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={std_val:.4f}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ target
    target_distribution = df_features['target'].value_counts()
    logging.info(f"ğŸ¯ ØªÙˆØ²ÛŒØ¹ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù: {target_distribution.to_dict()}")
    target_percentage = (target_distribution.get(1, 0) / len(df_features)) * 100
    logging.info(f"ğŸ“Š Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª: {target_percentage:.2f}%")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ
    logging.info("ğŸ’¾ Ø´Ø±ÙˆØ¹ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ...")
    os.makedirs(output_path, exist_ok=True)
    timestamp_str = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ
    output_filename = f'final_dataset_for_training_{timestamp_str}.parquet'
    output_file_path = os.path.join(output_path, output_filename)
    df_features.to_parquet(output_file_path)
    logging.info(f"âœ… Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± Ù…Ø³ÛŒØ± '{output_file_path}' (ÙØ±Ù…Øª Parquet) Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ CSV Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªÛŒ
    csv_output_filename = f'final_dataset_for_training_{timestamp_str}.csv'
    csv_output_file_path = os.path.join(output_path, csv_output_filename)
    sample_size = min(1000, len(df_features))
    df_features.head(sample_size).to_csv(csv_output_file_path, index=True)
    logging.info(f"ğŸ“„ Ù†Ù…ÙˆÙ†Ù‡ CSV ({sample_size:,} Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„) Ø¯Ø± Ù…Ø³ÛŒØ± '{csv_output_file_path}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§
    end_time = datetime.now()
    execution_time = end_time - start_time
    logging.info(f"â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ú©Ù„: {execution_time}")
    
    # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    print("\n" + "="*80)
    print("ğŸ‰ === Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡) ===")
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {final_rows:,}")
    print(f"ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(feature_columns)}")
    print(f"ğŸ¯ Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª: {target_percentage:.2f}%")
    print(f"â±ï¸ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {execution_time}")
    print(f"ğŸ“ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: {output_filename}")
    print("\nğŸ†• ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Broadcasting):")
    print("  âœ… sentiment_score (Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡ Ø§Ø² Broadcasting)")
    print("  âœ… sentiment_momentum (ØªØºÛŒÛŒØ±Ø§Øª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡)")
    print("  âœ… sentiment_ma_7 (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© 7 ÙˆØ§Ø­Ø¯)")
    print("  âœ… sentiment_ma_14 (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© 14 ÙˆØ§Ø­Ø¯)")
    print("  âœ… sentiment_volume (Ø­Ø¬Ù… ØªÙ‚Ø±ÛŒØ¨ÛŒ)")
    print("  âœ… sentiment_divergence (ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø§Ø² Ù‚ÛŒÙ…Øª)")
    print("="*80)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    if final_rows > 0:
        print("\n--- Ù†Ù…ÙˆÙ†Ù‡ Ûµ Ø±Ø¯ÛŒÙ Ø¢Ø®Ø± Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ ---")
        display_cols = ['open', 'high', 'low', 'close', 'volume', 'target'] + \
                      [col for col in ['sentiment_score', 'rsi', 'macd', 'bb_position'] if col in df_features.columns][:4]
        print(df_features[display_cols].tail())
        
        print(f"\n--- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ ---")
        print(f"Shape: {df_features.shape}")
        print(f"Memory usage: {df_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

if __name__ == '__main__':
    run_feature_engineering(PROCESSED_DATA_PATH, FEATURES_PATH)